{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2975fb98-a6fe-4b98-8872-c11cbf0a33f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved up\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "print(\"Moved up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58afea8-beda-4db5-8c9e-68237dff61e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 12:24:57.460904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746444297.474260 1406811 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746444297.478294 1406811 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devices [CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import copy\n",
    "import pickle\n",
    "import typing as tp\n",
    "from functools import partial\n",
    "\n",
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.flatten_util as fu\n",
    "from flax import linen as nn  # Linen API\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import lib_data\n",
    "import utils\n",
    "import modules\n",
    "import callbacks\n",
    "\n",
    "# %env XLA_PYTHON_CLIENT_MEM_FRACTION=.9\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print(\"devices\", jax.devices())\n",
    "\n",
    "shade_colours = plt.get_cmap('Set3')\n",
    "dark_colours = plt.get_cmap('tab10')\n",
    "all_colours = plt.get_cmap('tab20')\n",
    "\n",
    "def light_colours(i):\n",
    "    return all_colours(2*i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e68954-02f0-4f13-bc71-8fc7b3472765",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3e3216-8f97-4f75-8ed5-db3f36752aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙Tokenizing raw Wikitext-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2b8274cbd847778c93818943904b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299fda6bb89c411da66c7430543a6a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19204703365c400cb0803d99cb729abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenized dataset to: ./cached_wikitext2/tokenized_gpt2\n",
      "Building LM datasets...\n",
      "Flattening token sequences...\n",
      "Total 2391884 tokens. Creating 2335 chunks with stride 1024\n",
      "Flattening token sequences...\n",
      "Total 283287 tokens. Creating 276 chunks with stride 1024\n",
      "Train: 2335  Eval: 276  Hess: 2335\n",
      "Input shape: (1023,) Target shape: (1023,)\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "n_out = 1\n",
    "\n",
    "n_train: int = 100000\n",
    "n_eval: int = 10000\n",
    "n_hess: int = 1\n",
    "\n",
    "importlib.reload(lib_data)\n",
    "def __get_datasets():\n",
    "    datasets = lib_data.get_wikitext2_dataset(block_size=1024, max_train_samples=n_train, max_eval_samples=n_eval)\n",
    "\n",
    "    print(\"Train:\", len(datasets[0]), \" Eval:\", len(datasets[1]), \" Hess:\", len(datasets[2]))\n",
    "    x, y = datasets[0][0]\n",
    "    print(\"Input shape:\", x.shape, \"Target shape:\", y.shape)\n",
    "\n",
    "    data_name = \"wiki2_\"+str(n_out)+\"cl_\"+str(n_train) + \"_\" + str(n_eval) \n",
    "    # print(len(datasets))\n",
    "    return datasets, data_name\n",
    "\n",
    "datasets, data_name = __get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1825cf-cf66-4628-b1df-0fd549e6b388",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847ce83c-7ef5-4b66-90ea-57ea34b43343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "importlib.reload(modules)\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    config: ConfigDict\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        features = x.shape[-1]\n",
    "        x = nn.LayerNorm(dtype=self.config.dtype)(x)\n",
    "        x = nn.Dense(self.config.mlp_expansion * features, dtype=self.config.dtype)(x)\n",
    "        x = nn.gelu(x, approximate=True)\n",
    "        x = nn.Dense(features, dtype=self.config.dtype)(x)\n",
    "        x = nn.Dropout(rate=self.config.dropout_rate)(x, deterministic=not self.train)\n",
    "        return x\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    config: ConfigDict\n",
    "    mask: tp.Optional[jax.Array]\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        features = x.shape[-1]\n",
    "        x = nn.LayerNorm(dtype=self.config.dtype)(x)\n",
    "        qkv = nn.DenseGeneral(\n",
    "            features=(self.config.num_heads, self.config.head_dim * 3),\n",
    "            axis=-1, dtype=self.config.dtype\n",
    "        )(x)\n",
    "        q, k, v = jnp.split(qkv, 3, axis=-1)\n",
    "\n",
    "        scale = q.shape[-1] ** -0.5\n",
    "        q = q.astype(self.config.softmax_dtype) * scale\n",
    "        k = k.astype(self.config.softmax_dtype)\n",
    "\n",
    "        q = q.transpose(0, 2, 1, 3) # [B T H D] to [B H T D]\n",
    "        k = k.transpose(0, 2, 1, 3) # [B T H D] to [B H T D]\n",
    "        v = v.transpose(0, 2, 1, 3) # [B T H D] to [B H T D]\n",
    "    \n",
    "        attn = q @ k.swapaxes(-2, -1) # [B H T D] @ [B H D T] -> [B H T T]\n",
    "    \n",
    "        if self.mask is not None:\n",
    "            attn = jnp.where(self.mask, attn, jnp.finfo(self.config.softmax_dtype).min)\n",
    "    \n",
    "        attn = nn.softmax(attn, axis=-1).astype(self.config.dtype)\n",
    "        attn = nn.Dropout(rate=self.config.dropout_rate)(attn, deterministic=not self.train)\n",
    "        y = attn @ v # [B H T T] @ [B H T D] -> [B H T D]\n",
    "        y = y.transpose(0, 2, 1, 3) # [B H T D] -> [B T H D]\n",
    "        y = y.reshape(x.shape)  # [B T H D] -> [B T C(H*D)]\n",
    "        y = nn.Dense(features, dtype=self.config.dtype)(y)\n",
    "        y = nn.Dropout(rate=self.config.dropout_rate)(y, deterministic=not self.train)\n",
    "        return y\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    config: ConfigDict\n",
    "    mask: tp.Optional[jax.Array]\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        mlp = MLPBlock\n",
    "        if \"MLP\" in self.config.remat:\n",
    "            mlp = nn.remat(mlp, prevent_cse=False)\n",
    "        attn = AttentionBlock\n",
    "        if \"Attn\" in self.config.remat:\n",
    "            attn = nn.remat(attn, prevent_cse=False)\n",
    "\n",
    "        x = x + attn(config=self.config, mask=self.mask, train=self.train)(x)\n",
    "        x = x + mlp(config=self.config, train=self.train)(x)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    config: ConfigDict\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask=None, train=True):\n",
    "        if mask is None and self.config.causal_mask:\n",
    "            mask = nn.make_causal_mask(x, dtype=jnp.bool_)\n",
    "\n",
    "        embed = nn.Embed(self.config.vocab_size, self.config.hidden_size, dtype=self.config.dtype, name='token_embed')\n",
    "        x = embed(x) \n",
    "        pos_emb = self.param(\"pos_emb\", nn.initializers.normal(0.02),\n",
    "                             (self.config.max_seq_len, self.config.hidden_size)).astype(self.config.dtype)\n",
    "        \n",
    "        x += pos_emb[None, :x.shape[1]]\n",
    "\n",
    "        block_fn = functools.partial(TransformerBlock, config=self.config, mask=mask, train=train)\n",
    "\n",
    "        if self.config.scan_layers:\n",
    "            block = block_fn(name=\"block\")\n",
    "            x, _ = nn.scan(\n",
    "                lambda module, carry, _: (module(carry), None),\n",
    "                variable_axes={\"params\": 0},\n",
    "                split_rngs={\"params\": True, \"dropout\": True},\n",
    "                length=self.config.num_layers\n",
    "            )(block, x, ())\n",
    "        else:\n",
    "            for i in range(self.config.num_layers):\n",
    "                x = block_fn(name=f\"block_{i}\")(x)\n",
    "\n",
    "        x = nn.LayerNorm(dtype=self.config.dtype)(x)\n",
    "\n",
    "        # weight tying\n",
    "        logits = x @ embed.embedding.T\n",
    "        return logits.astype(jnp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04812627-f221-48e2-95e7-471b3561b19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer_L6_H384_Heads6\n"
     ]
    }
   ],
   "source": [
    "config = ConfigDict()\n",
    "config.vocab_size = 50257\n",
    "config.hidden_size = 384\n",
    "config.num_layers = 6\n",
    "config.num_heads = 6\n",
    "config.head_dim = 64\n",
    "config.mlp_expansion = 4\n",
    "config.dropout_rate = 0.1\n",
    "config.max_seq_len = 1024\n",
    "config.num_outputs = 50257\n",
    "config.dtype = jnp.float32\n",
    "config.causal_mask = True\n",
    "config.softmax_dtype = jnp.float32\n",
    "config.remat = [],  #[\"MLP\", \"Attn\"]\n",
    "config.scan_layers = False\n",
    "\n",
    "def __get_arch__():\n",
    "\n",
    "    model = Transformer(config)\n",
    "    model_name = f\"Transformer_L{config.num_layers}_H{config.hidden_size}_Heads{config.num_heads}\"\n",
    "    return model, model_name\n",
    "\n",
    "model_arch, model_name = __get_arch__()\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "807718aa-21fe-4d29-a2ca-b931057d684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = ConfigDict()\n",
    "# config.vocab_size = 50257\n",
    "# config.hidden_size = 768\n",
    "# config.num_layers = 12\n",
    "# config.num_heads = 12\n",
    "# config.head_dim = 64\n",
    "# config.mlp_expansion = 4\n",
    "# config.dropout_rate = 0.1\n",
    "# config.max_seq_len = 1024\n",
    "# config.num_outputs = 50257\n",
    "# config.dtype = jnp.float32\n",
    "# config.causal_mask = True\n",
    "# config.softmax_dtype = jnp.float32\n",
    "# config.remat = [\"MLP\", \"Attn\"]\n",
    "# config.scan_layers = False\n",
    "\n",
    "# def __get_arch__():\n",
    "\n",
    "#     model = Transformer(config)\n",
    "#     model_name = f\"Transformer_L{config.num_layers}_H{config.hidden_size}_Heads{config.num_heads}\"\n",
    "#     return model, model_name\n",
    "\n",
    "# model_arch, model_name = __get_arch__()\n",
    "# print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d222c-9284-48b2-ba72-6ae1cf02ef3d",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f871d6d-d289-4915-8975-f9c44d058baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "importlib.reload(modules)\n",
    "from optax import contrib\n",
    "\n",
    "def __get_optim__(warmup_steps, lr, b1, b2, b3, option=\"\", rho=None, sync_period=1):\n",
    "    # warmup_steps, lr, b1, b2, b3 = hyps['warmup_steps'], hyps['lr'], hyps['b1'], hyps['b2'], hyps['b3']\n",
    "    if option == 'sam':\n",
    "        assert rho is not None\n",
    "        warmup_scheduler = optax.linear_schedule(init_value=0.0, end_value=lr,\n",
    "                                                transition_steps=warmup_steps,\n",
    "                                                transition_begin=0,)\n",
    "        constant_scheduler = optax.constant_schedule(lr)\n",
    "        lr_scheduler = optax.join_schedules([warmup_scheduler, constant_scheduler], boundaries=[warmup_steps])\n",
    "        base_opt = modules.get_sgd_optimizer(lr_scheduler, b1, b2, b3, verbose=False)\n",
    "        adv_opt = modules.get_sgd_optimizer(rho, b1, b2, b3, verbose=False)\n",
    "        optimizer = contrib.sam(base_opt, adv_opt, sync_period=sync_period, opaque_mode=True) # sam opt\n",
    "        optim_name = f\"sgdFam-SAM_1b{b1}_2b{b2}_3b{b3}_lr{lr}_warmup{warmup_steps}_rho{rho}_syncT{sync_period}\"\n",
    "\n",
    "    else:\n",
    "        warmup_scheduler = optax.linear_schedule(init_value=0.0, end_value=lr,\n",
    "                                                transition_steps=warmup_steps,\n",
    "                                                transition_begin=0,)\n",
    "        constant_scheduler = optax.constant_schedule(lr)\n",
    "        lr_scheduler = optax.join_schedules([warmup_scheduler, constant_scheduler], boundaries=[warmup_steps])\n",
    "        optimizer = modules.get_sgd_optimizer(lr_scheduler, b1, b2, b3, verbose=False)\n",
    "        optim_name = f\"sgdFam_1b{b1}_2b{b2}_3b{b3}_lr{lr}_warmup{warmup_steps}\"\n",
    "        \n",
    "    return optimizer, optim_name\n",
    "\n",
    "optimizer, optim_name = __get_optim__(2, 0.1, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae0321-a5a3-4190-9934-f87074466230",
   "metadata": {},
   "source": [
    "# Model Params (Fixed and Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bba4401-c995-444b-8b5a-fcf6efdb42e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "import itertools\n",
    "### FIXED\n",
    "warmup_steps = 2\n",
    "bs = 1\n",
    "eval_bs = 1\n",
    "n_epochs = 10\n",
    "loss_fn = optax.softmax_cross_entropy_with_integer_labels\n",
    "\n",
    "# FLEXIBLE\n",
    "# lr = 0.005\n",
    "# beta_list = [(0., 0., 0.), (0., 0.99, 0.)]\n",
    "# optim_hp = LR, B1, B2, B3, SAM, Rho, sync_period\n",
    "optim_hp_list = [\n",
    "    (5e-3, 0.9, 0.99, 0., False, 0., 1), \n",
    "]\n",
    "seed_list = [x for x in range(1)]\n",
    "# sam_list = [False, True]\n",
    "s = [optim_hp_list, seed_list]\n",
    "hyp_list = list(itertools.product(*s))\n",
    "print(len(hyp_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a3fe1b-9c7b-4df5-95c2-74d36952280c",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f047b6bf-6631-4765-b580-5729e89aa0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "sws = 5\n",
    "cb_freq = 1\n",
    "hess_freq = int(1e8) # really large\n",
    "importlib.reload(callbacks)\n",
    "def __get_cbs__(state, compute_hessian=False):\n",
    "    cbs = []\n",
    "    cbs.append(callbacks.saveWeightsCB(sws, grad=True))\n",
    "    # cbs.append(callbacks.thinCB(thin_freq=cb_freq))\n",
    "\n",
    "    if compute_hessian:\n",
    "        hvpCB = callbacks.hvpCB(loss_fn=loss_fn, batches=(datasets[2].data[:n_hess], datasets[2].targets[:n_hess]), \n",
    "                            save_freq=hess_freq, hess_bs=n_hess, state=state, bn=False)\n",
    "        cbs.append(hvpCB)   \n",
    "        specCB = callbacks.spectrumCB(n_eigs=20, n_evecs=10, \n",
    "                    loss_fn=loss_fn, seed=seed, hvpCB=hvpCB, save_freq=hess_freq, verbose=False)\n",
    "        cbs.append(specCB)\n",
    "\n",
    "        esCB = callbacks.earlyStopCB(acc_threshold=0.999, cbs=None, min_eps=sws, max_eps=n_epochs,conseq_eps=3,\n",
    "                                 final_cbs=[hvpCB, specCB], verbose=False, low_eps=max(sws, 100), low_thresh=0.11, )\n",
    "    else:\n",
    "        esCB = callbacks.earlyStopCB(acc_threshold=0.999, cbs=None, min_eps=sws, max_eps=n_epochs, conseq_eps=5,\n",
    "                                 verbose=False, low_eps=max(sws, 100), low_thresh=0., )\n",
    "    cbs.append(esCB)\n",
    "    return cbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62ae84-b4c1-4432-b483-740b815ac760",
   "metadata": {},
   "source": [
    "# Train State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a0d986-e610-48b1-8b2b-837cd218d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import struct                # Flax dataclasses\n",
    "from clu import metrics\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "from perplexity import Perplexity\n",
    "importlib.reload(modules)\n",
    "\n",
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    accuracy: metrics.Accuracy\n",
    "    perplexity: Perplexity\n",
    "    loss: metrics.Average.from_output('loss')\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    rng: jax.Array\n",
    "\n",
    "class TrainStateBN(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    batch_stats: tp.Any\n",
    "    rng: jax.Array\n",
    "\n",
    "class TrainStateSAM(modules.TrainStateSAM):\n",
    "    metrics: Metrics\n",
    "    batch_stats: tp.Any\n",
    "    rng: jax.Array\n",
    "\n",
    "def create_train_state(model, optimizer, inputs, rng, option=\"\"):\n",
    "    \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "    rng, model_rng = jax.random.split(rng)\n",
    "    if option == \"\":\n",
    "        params = model.init(model_rng, jnp.ones_like(inputs[0][jnp.newaxis, :]))['params'] # initialize parameters by passing a template image\n",
    "        \n",
    "        tx = optimizer\n",
    "        return TrainState.create(\n",
    "          apply_fn=model.apply, params=params, tx=tx, metrics=Metrics.empty(), rng=rng)\n",
    "        \n",
    "    elif option == \"bn\":\n",
    "        variables = model.init(model_rng, jnp.ones_like(inputs[0][jnp.newaxis, :])) # initialize parameters by passing a template image\n",
    "        params = variables['params']\n",
    "        batch_stats = variables['batch_stats']\n",
    "        \n",
    "        tx = optimizer\n",
    "        return TrainStateBN.create(\n",
    "          apply_fn=model.apply, params=params, tx=tx, batch_stats=batch_stats, \n",
    "          metrics=Metrics.empty(), rng=rng)\n",
    "        \n",
    "    elif option == \"sam\":\n",
    "        variables = model.init(model_rng, jnp.ones_like(inputs[0][jnp.newaxis, :])) # initialize parameters by passing a template image\n",
    "        params = variables['params']\n",
    "        batch_stats = variables['batch_stats']\n",
    "        \n",
    "        tx = optimizer\n",
    "        return TrainStateSAM.create(\n",
    "          apply_fn=model.apply, params=params, tx=tx, batch_stats=batch_stats, \n",
    "          metrics=Metrics.empty(), rng=rng)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98af9c-1eaf-4282-a98f-a6e2a910dffe",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f060f232-f47d-4721-92a0-c6309ff6f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "# from flax.linen import tabulate\n",
    "# tabulated_fn = tabulate(model, rngs={\"params\": jax.random.PRNGKey(0), \"dropout\": jax.random.PRNGKey(1)},\n",
    "#                            console_kwargs={\"width\": 200, \"force_jupyter\": False}) # Avoid output clipping in notebooks)\n",
    "\n",
    "# print(tabulated_fn(sample_batch[0], train=False))\n",
    "# # with open(\"gpt2_summary.txt\", \"w\") as f:\n",
    "#     # f.write(tab_fn(dummy_input, train=False))\n",
    "#     # f.write(tabulated_fn(sample_batch[0], train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ca04fea-3cd2-4bb9-a280-64798d3ae2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def _compute_metrics(*, state, batch):\n",
    "    preds = state.apply_fn({'params': state.params}, batch[0], train=False)\n",
    "    loss = loss_fn(preds, batch[1]).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(\n",
    "        logits=preds, labels=batch[1], loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a09ed9b5-aa57-4e9c-8718-94529a54ca9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num params 30339456\n",
      "output dim (1, 1023, 50257)\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "import training\n",
    "importlib.reload(training)\n",
    "\n",
    "load_files = False\n",
    "compute_hessian = False\n",
    "force_train = True\n",
    "\n",
    "all_mh = []\n",
    "all_exp_names = []\n",
    "\n",
    "for hyp in hyp_list:\n",
    "    \n",
    "    metrics_history = {'train_loss': [],\n",
    "                   'train_accuracy': [],\n",
    "                   'train_perplexity': [],\n",
    "                   'test_loss': [],\n",
    "                   'test_accuracy': [],\n",
    "                   'test_perplexity': [],\n",
    "                      }\n",
    "\n",
    "    lr, b1, b2, b3, sam, sam_rho, sync_T = hyp[0]\n",
    "    seed = hyp[1]\n",
    "    option = 'sam' if sam else \"\"\n",
    "    \n",
    "    if datasets is None:\n",
    "        datasets, data_name = __get_datasets__()\n",
    "    \n",
    "    train_loader = lib_data.NumpyLoader(datasets[0], batch_size=bs, shuffle=False)\n",
    "    for sample_batch in train_loader:\n",
    "        break\n",
    "    \n",
    "    test_loader = lib_data.NumpyLoader(datasets[1], batch_size=eval_bs)\n",
    "    dataloaders = [train_loader, test_loader]\n",
    "    \n",
    "    model, model_name = __get_arch__()\n",
    "    model_name += \"_seed\"+str(seed)\n",
    "\n",
    "    optim, optim_name = __get_optim__(warmup_steps, lr, b1, b2, b3, option=option, rho=sam_rho, sync_period=sync_T)\n",
    "    optim_name += f\"_epochs{n_epochs}_bs{bs}\"\n",
    "\n",
    "    init_rng = jax.random.PRNGKey(seed)\n",
    "    state = create_train_state(model, optim, sample_batch[0], init_rng, option=option)\n",
    "    del init_rng  # Must not be used anymore.\n",
    "    num_params = utils.count_params(state.params)\n",
    "    print(\"num params\", num_params)\n",
    "    \n",
    "    # state = load_params(state, config)\n",
    "    \n",
    "    sample_out = state.apply_fn({'params': state.params,}, sample_batch[0], train=False)\n",
    "    print(\"output dim\", sample_out.shape)\n",
    "\n",
    "    break\n",
    "\n",
    "    # evaluate perplexity\n",
    "\n",
    "    # train_bar = tqdm(train_loader, desc='train', total=len(train_loader))\n",
    "    # test_bar = tqdm(test_loader, desc=\"validation\", total=len(test_loader))\n",
    "\n",
    "    # for batch in train_bar:\n",
    "    #     state = _compute_metrics(state=state, batch=batch)\n",
    "    # for metric, value in state.metrics.compute().items():  # compute metrics\n",
    "    #     metrics_history[f'train_{metric}'].append(value)  # record metrics\n",
    "    # utils.reset_metrics(state)\n",
    "    \n",
    "    # for batch in test_bar:\n",
    "    #     state = _compute_metrics(state=state, batch=batch)\n",
    "    # for metric, value in state.metrics.compute().items():  # compute metrics\n",
    "    #     metrics_history[f'test_{metric}'].append(value)  # record metrics\n",
    "    # utils.reset_metrics(state)\n",
    "    # print(metrics_history)\n",
    "    \n",
    "    # train model\n",
    "    cbs = __get_cbs__(state, compute_hessian=compute_hessian)\n",
    "    cb_name_str = utils.get_callback_name_str(cbs)\n",
    "    cb_name_list = utils.get_callback_name_list(cbs)\n",
    "    # break\n",
    "\n",
    "    experiment_name = utils.get_now() + \"_\" + data_name + \"_\" + model_name + \"_\" + optim_name\n",
    "    \n",
    "    try:\n",
    "        if force_train:\n",
    "            raise FileNotFoundError\n",
    "        experiment_name, lse = utils.find_latest_exp(experiment_name, n_epochs, save_freq=cb_freq, \n",
    "                                                   cbs=cb_name_list, unknown_lse=True, verbose=False)\n",
    "        metrics_history = utils.load_thing(\"traj/\" + experiment_name + \"/metrics.pkl\")\n",
    "        print(f\"tr_acc: {metrics_history['train_accuracy'][-1]:0%}, te_acc: {metrics_history['test_accuracy'][-1]:0%}\")\n",
    "        metrics_history['lse'] = [lse]\n",
    "        if compute_hessian:\n",
    "            eigvals = utils.load_thing(\"traj/\" + experiment_name + \"/eigvals.pkl\")\n",
    "            metrics_history['eigvals'] = eigvals\n",
    "            print(f\"sharp: {metrics_history['eigvals'][-1][0]}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        metrics_history = training.train_model(state, model, loss_fn, metrics_history, n_epochs, dataloaders, \\\n",
    "                                                   experiment_name, cbs, option=option, force_fb=False, tqdm_over_epochs=1, \n",
    "                                              eval_freq=1, gradient_accumulation=32, tqdm_over_batch=False)         \n",
    "        \n",
    "    all_mh.append(metrics_history)\n",
    "    all_exp_names.append(experiment_name)\n",
    "    \n",
    "    print(experiment_name, \"complete\")\n",
    "    print(\"\\n ---------------------------------------------------------------------------------------------------------\\n\")\n",
    "# Training: datasets, hps, arch_func, optim_func, cb_func, -> train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b6f25d-349d-45b0-877b-c5fd43f1b000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee92ee6-f8fa-4ecd-96ac-9db481096fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218f96a-83d8-41ba-913e-b02344bc19fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71eb9178-e47b-45cf-ada0-bbbc84bc7342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output dim (1, 127, 50257)\n",
      "num params 124439808\n",
      "Training model 250501-1848_wiki2_1cl_10_10_Transformer_L12_H768_Heads12_seed0_sgdFam_1b0.9_2b0.99_3b0.0_lr0.005_warmup2_epochs0_bs1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73882d6855a43acab7c2fac39bd0da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete 250501-1848_wiki2_1cl_10_10_Transformer_L12_H768_Heads12_seed0_sgdFam_1b0.9_2b0.99_3b0.0_lr0.005_warmup2_epochs0_bs1\n",
      "250501-1848_wiki2_1cl_10_10_Transformer_L12_H768_Heads12_seed0_sgdFam_1b0.9_2b0.99_3b0.0_lr0.005_warmup2_epochs0_bs1 complete\n",
      "\n",
      " ---------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "import training\n",
    "importlib.reload(training)\n",
    "\n",
    "load_files = False\n",
    "compute_hessian = False\n",
    "force_train = True\n",
    "\n",
    "all_mh = []\n",
    "all_exp_names = []\n",
    "\n",
    "for hyp in hyp_list:\n",
    "    \n",
    "    metrics_history = {'train_loss': [],\n",
    "                   'train_accuracy': [],\n",
    "                   'train_perplexity': [],\n",
    "                   'test_loss': [],\n",
    "                   'test_accuracy': [],\n",
    "                   'test_perplexity': [],\n",
    "                      }\n",
    "\n",
    "    lr, b1, b2, b3, sam, sam_rho, sync_T = hyp[0]\n",
    "    seed = hyp[1]\n",
    "    option = 'sam' if sam else \"\"\n",
    "    \n",
    "    if datasets is None:\n",
    "        datasets, data_name = __get_datasets__()\n",
    "    \n",
    "    train_loader = lib_data.NumpyLoader(datasets[0], batch_size=bs, shuffle=True)\n",
    "    for sample_batch in train_loader:\n",
    "        break\n",
    "    \n",
    "    test_loader = lib_data.NumpyLoader(datasets[1], batch_size=eval_bs)\n",
    "    dataloaders = [train_loader, test_loader]\n",
    "    \n",
    "    model, model_name = __get_arch__()\n",
    "    model_name += \"_seed\"+str(seed)\n",
    "\n",
    "    optim, optim_name = __get_optim__(warmup_steps, lr, b1, b2, b3, option=option, rho=sam_rho, sync_period=sync_T)\n",
    "    optim_name += f\"_epochs{n_epochs}_bs{bs}\"\n",
    "\n",
    "    init_rng = jax.random.PRNGKey(seed)\n",
    "    state = create_train_state(model, optim, sample_batch[0], init_rng, option=option)\n",
    "    del init_rng  # Must not be used anymore.\n",
    "\n",
    "    state = load_params(state)\n",
    "    \n",
    "    sample_out = state.apply_fn({'params': state.params,}, sample_batch[0], train=False)\n",
    "    print(\"output dim\", sample_out.shape)\n",
    "    # break\n",
    "    cbs = __get_cbs__(state, compute_hessian=compute_hessian)\n",
    "    cb_name_str = utils.get_callback_name_str(cbs)\n",
    "    cb_name_list = utils.get_callback_name_list(cbs)\n",
    "    # break\n",
    "    num_params = utils.count_params(state.params)\n",
    "    print(\"num params\", num_params)\n",
    "\n",
    "    experiment_name = utils.get_now() + \"_\" + data_name + \"_\" + model_name + \"_\" + optim_name\n",
    "    \n",
    "    try:\n",
    "        if force_train:\n",
    "            raise FileNotFoundError\n",
    "        experiment_name, lse = utils.find_latest_exp(experiment_name, n_epochs, save_freq=cb_freq, \n",
    "                                                   cbs=cb_name_list, unknown_lse=True, verbose=False)\n",
    "        metrics_history = utils.load_thing(\"traj/\" + experiment_name + \"/metrics.pkl\")\n",
    "        print(f\"tr_acc: {metrics_history['train_accuracy'][-1]:0%}, te_acc: {metrics_history['test_accuracy'][-1]:0%}\")\n",
    "        metrics_history['lse'] = [lse]\n",
    "        if compute_hessian:\n",
    "            eigvals = utils.load_thing(\"traj/\" + experiment_name + \"/eigvals.pkl\")\n",
    "            metrics_history['eigvals'] = eigvals\n",
    "            print(f\"sharp: {metrics_history['eigvals'][-1][0]}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        metrics_history = training.train_model(state, model, loss_fn, metrics_history, n_epochs, dataloaders, \\\n",
    "                                                   experiment_name, cbs, option=option, force_fb=False, tqdm_over_epochs=1, \n",
    "                                              eval_freq=1, gradient_accumulation=1)         \n",
    "        \n",
    "    all_mh.append(metrics_history)\n",
    "    all_exp_names.append(experiment_name)\n",
    "    \n",
    "    print(experiment_name, \"complete\")\n",
    "    print(\"\\n ---------------------------------------------------------------------------------------------------------\\n\")\n",
    "# Training: datasets, hps, arch_func, optim_func, cb_func, -> train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba3000e4-7a4f-46fb-86db-389d1f8e5ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': [Array(4.0963697, dtype=float32)], 'train_accuracy': [Array(0.28031495, dtype=float32)], 'train_perplexity': [Array(60.12163, dtype=float32)], 'test_loss': [Array(4.162267, dtype=float32)], 'test_accuracy': [Array(0.32283464, dtype=float32)], 'test_perplexity': [Array(64.21695, dtype=float32)], 'lse': 0}\n"
     ]
    }
   ],
   "source": [
    "print(all_mh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18481a5c-d4b0-4994-9ff1-6fba0e8354bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " of the is not\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "input_ids = tokenizer(\"The meaning of life is\", return_tensors=\"jax\").input_ids\n",
    "out = state.apply_fn({'params': state.params}, input_ids, train=False)\n",
    "decoded = tokenizer.decode(jnp.argmax(out, axis=-1)[0])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5d41d63-7316-4bb0-9d71-3d2425ec795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 127)\n",
      " game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi\n"
     ]
    }
   ],
   "source": [
    "# print(sample_out.shape)\n",
    "token_ids = sample_batch[0]\n",
    "# token_ids = jnp.argmax(nn.softmax(sample_out), axis=-1)  # shape: [batch_size, seq_len]\n",
    "print(token_ids.shape)\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "decoded = tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n",
    "for text in decoded:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a9f335bb-85e6-4c44-ad9f-5e47b2ec6e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=               | �---\n",
      "Valkyria        | polls--\n",
      "Chronicles      | Democrats\n",
      "III             | ------\n",
      "=               | Democrats-------------\n",
      "Senjō           | Democratsn----\n",
      "no              | Democrats--\n",
      "Valkyria        | Democrats\n",
      "3               | Democratsn-------\n",
      ":               | -------\n",
      "Unrecorded      | --\n",
      "Chronicles      | --\n",
      "(               | -\n",
      "Japanese        | b\n",
      ":               | ----\n",
      "戦場のヴァルキュリア3     | -\n",
      ",               | -\n",
      "lit             | -\n",
      ".               | --\n",
      "Valkyria        | --\n",
      "of              | -\n",
      "the             | -\n",
      "Battlefield     | -\n",
      "3               | --\n",
      ")               | -\n",
      ",               | -\n",
      "commonly        | --\n",
      "referred        | we\n",
      "to              | --\n",
      "as              | we-b\n",
      "Valkyria        | -\n",
      "Chronicles      | --\n",
      "III             | �\n",
      "outside         | -\n",
      "Japan           | -\n",
      ",               | �\n",
      "is              | �\n",
      "a               | -\n",
      "tactical        | �\n",
      "role            | we\n",
      "@-@             | I\n",
      "playing         | we\n",
      "video           | -\n",
      "game            | �\n",
      "developed       | �\n",
      "by              | �\n",
      "Sega            | I\n",
      "and             | -\n",
      "Media.Vision    | we-\n",
      "for             | few-\n",
      "the             | we\n",
      "PlayStation     | �\n",
      "Portable        | ��-\n",
      ".               | �\n",
      "Released        | I\n",
      "in              | ..\n",
      "January         | -��-\n",
      "2011            | we-\n",
      "in              | I�-�-\n",
      "Japan           | �\n",
      ",               | we--.\n",
      "it              | �.-��n���-��\n",
      "is              | �\n",
      "the             | .-\n",
      "third           | I.�.��-��.�-�.�\n",
      "game            | 1-��.-\n",
      "in              | �\n",
      "the             | �1\n",
      "Valkyria        | -�-.-�1�\n",
      "series          | �\n",
      ".               | �-�-.-���-�-.���.\n",
      "Employing       | -���\n",
      "the             | we.��.��-�-����\n",
      "same            | �n--����n���\n",
      "fusion          | -�-������.�������..��\n",
      "of              | few����\n",
      "tactical        | last�1�-���-�\n",
      "and             | �\n",
      "real            | �-��\n",
      "@-@             | last��\n",
      "time            | last��������-\n",
      "gameplay        | last�-\n",
      "as              | �\n",
      "its             | �������\n",
      "predecessors    | .�.������������\n",
      ",               | few���1��������\n",
      "the             | few.���--.���\n",
      "story           | ��.-���1����.�����..-.����-����1.�.-\n",
      "runs            | few��-.�.�..��-��-�-.����1.��-�����\n",
      "parallel        | \n",
      "to              | \n",
      "the             | \n",
      "first           | \n",
      "game            | \n",
      "and             | \n",
      "follows         | \n",
      "the             | \n",
      "\"               | \n",
      "Nameless        | \n",
      "\"               | \n",
      ",               | \n",
      "a               | \n",
      "penal           | \n",
      "military        | \n",
      "unit            | \n",
      "serving         | \n",
      "the             | \n",
      "nation          | \n",
      "of              | \n",
      "Gallia          | \n",
      "during          | \n",
      "the             | \n",
      "Second          | \n",
      "Europan         | \n",
      "War             | \n",
      "who             | \n",
      "perform         | \n",
      "secret          | \n",
      "black           | \n",
      "operations      | \n",
      "and             | \n",
      "are             | \n",
      "pitted          | \n",
      "against         | \n",
      "the             | \n",
      "Imperial        | \n",
      "unit            | \n",
      "\"               | \n",
      "Calamaty        | \n",
      "Raven           | \n",
      "\"               | \n",
      ".               | \n",
      "The             | \n",
      "game            | \n",
      "began           | \n",
      "development     | \n",
      "in              | \n",
      "2010            | \n",
      ",               | \n",
      "carrying        | \n",
      "over            | \n",
      "a               | \n",
      "large           | \n",
      "portion         | \n",
      "of              | \n",
      "the             | \n",
      "work            | \n",
      "done            | \n",
      "on              | \n",
      "Valkyria        | \n",
      "Chronicles      | \n",
      "II              | \n",
      ".               | \n",
      "While           | \n",
      "it              | \n",
      "retained        | \n",
      "the             | \n",
      "standard        | \n",
      "features        | \n",
      "of              | \n",
      "the             | \n",
      "series          | \n",
      ",               | \n",
      "it              | \n",
      "also            | \n",
      "underwent       | \n",
      "multiple        | \n",
      "adjustments     | \n",
      ",               | \n",
      "such            | \n",
      "as              | \n",
      "making          | \n",
      "the             | \n",
      "game            | \n",
      "more            | \n",
      "forgiving       | \n",
      "for             | \n",
      "series          | \n",
      "newcomers       | \n",
      ".               | \n",
      "Character       | \n",
      "designer        | \n",
      "Raita           | \n",
      "Honjou          | \n",
      "and             | \n",
      "composer        | \n",
      "Hitoshi         | \n",
      "Sakimoto        | \n",
      "both            | \n",
      "returned        | \n",
      "from            | \n",
      "previous        | \n",
      "entries         | \n",
      ",               | \n",
      "along           | \n",
      "with            | \n",
      "Valkyria        | \n",
      "Chronicles      | \n",
      "II              | \n",
      "director        | \n",
      "Takeshi         | \n",
      "Ozawa           | \n",
      ".               | \n",
      "A               | \n",
      "large           | \n",
      "team            | \n",
      "of              | \n",
      "writers         | \n",
      "handled         | \n",
      "the             | \n",
      "script          | \n",
      ".               | \n",
      "The             | \n",
      "game            | \n",
      "'s              | \n",
      "opening         | \n",
      "theme           | \n",
      "was             | \n",
      "sung            | \n",
      "by              | \n",
      "May             | \n",
      "'n              | \n",
      ".               | \n",
      "It              | \n",
      "met             | \n",
      "with            | \n",
      "positive        | \n",
      "sales           | \n",
      "in              | \n",
      "Japan           | \n",
      ",               | \n",
      "and             | \n",
      "was             | \n",
      "praised         | \n",
      "by              | \n",
      "both            | \n",
      "Japanese        | \n",
      "and             | \n",
      "western         | \n",
      "critics         | \n",
      ".               | \n",
      "After           | \n",
      "release         | \n",
      ",               | \n",
      "it              | \n",
      "received        | \n",
      "downloadable    | \n",
      "content         | \n",
      ",               | \n",
      "along           | \n",
      "with            | \n",
      "an              | \n",
      "expanded        | \n",
      "edition         | \n",
      "in              | \n",
      "November        | \n",
      "of              | \n",
      "that            | \n",
      "year            | \n",
      ".               | \n",
      "It              | \n",
      "was             | \n",
      "also            | \n",
      "adapted         | \n",
      "into            | \n",
      "manga           | \n",
      "and             | \n",
      "an              | \n",
      "original        | \n",
      "video           | \n",
      "animation       | \n",
      "series          | \n",
      ".               | \n",
      "Due             | \n",
      "to              | \n",
      "low             | \n",
      "sales           | \n",
      "of              | \n",
      "Valkyria        | \n",
      "Chronicles      | \n",
      "II              | \n",
      ",               | \n",
      "Valkyria        | \n",
      "Chronicles      | \n",
      "III             | \n",
      "was             | \n",
      "not             | \n",
      "localized       | \n",
      ",               | \n",
      "but             | \n",
      "a               | \n",
      "fan             | \n",
      "translation     | \n",
      "compatible      | \n",
      "with            | \n",
      "the             | \n",
      "game            | \n",
      "'s              | \n",
      "expanded        | \n",
      "edition         | \n",
      "was             | \n",
      "released        | \n",
      "in              | \n",
      "2014            | \n",
      ".               | \n",
      "Media.Vision    | \n",
      "would           | \n",
      "return          | \n",
      "to              | \n",
      "the             | \n",
      "franchise       | \n",
      "with            | \n",
      "the             | \n",
      "development     | \n",
      "of              | \n",
      "Valkyria        | \n",
      ":               | \n",
      "Azure           | \n",
      "Revolution      | \n",
      "for             | \n",
      "the             | \n",
      "PlayStation     | \n",
      "4               | \n",
      ".               | \n",
      "=               | \n",
      "=               | \n",
      "Gameplay        | \n",
      "=               | \n",
      "=               | \n",
      "As              | \n",
      "with            | \n",
      "previous        | \n",
      "Valkyira        | \n",
      "Chronicles      | \n",
      "games           | \n",
      ",               | \n",
      "Valkyria        | \n",
      "Chronicles      | \n",
      "III             | \n",
      "is              | \n",
      "a               | \n",
      "tactical        | \n",
      "role            | \n",
      "@-@             | \n",
      "playing         | \n",
      "game            | \n",
      "where           | \n",
      "players         | \n",
      "take            | \n",
      "control         | \n",
      "of              | \n",
      "a               | \n",
      "military        | \n",
      "unit            | \n",
      "and             | \n",
      "take            | \n",
      "part            | \n",
      "in              | \n",
      "missions        | \n",
      "against         | \n",
      "enemy           | \n",
      "forces          | \n",
      ".               | \n",
      "Stories         | \n",
      "are             | \n",
      "told            | \n",
      "through         | \n",
      "comic           | \n",
      "book            | \n",
      "@-@             | \n",
      "like            | \n",
      "panels          | \n",
      "with            | \n",
      "animated        | \n",
      "character       | \n",
      "portraits       | \n",
      ",               | \n",
      "with            | \n",
      "characters      | \n",
      "speaking        | \n",
      "partially       | \n",
      "through         | \n",
      "voiced          | \n",
      "speech          | \n",
      "bubbles         | \n",
      "and             | \n",
      "partially       | \n",
      "through         | \n",
      "unvoiced        | \n",
      "text            | \n",
      ".               | \n",
      "The             | \n",
      "player          | \n",
      "progresses      | \n",
      "through         | \n",
      "a               | \n",
      "series          | \n",
      "of              | \n",
      "linear          | \n",
      "missions        | \n",
      ",               | \n",
      "gradually       | \n",
      "unlocked        | \n",
      "as              | \n",
      "maps            | \n",
      "that            | \n",
      "can             | \n",
      "be              | \n",
      "freely          | \n",
      "scanned         | \n",
      "through         | \n",
      "and             | \n",
      "replayed        | \n",
      "as              | \n",
      "they            | \n",
      "are             | \n",
      "unlocked        | \n",
      ".               | \n",
      "The             | \n",
      "route           | \n",
      "to              | \n",
      "each            | \n",
      "story           | \n",
      "location        | \n",
      "on              | \n",
      "the             | \n",
      "map             | \n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# token_ids = sample_batch[0]\n",
    "token_ids_in = sample_batch[0]\n",
    "token_ids_out = jnp.argmax(nn.softmax(sample_out), axis=-1)  # shape: [batch_size, seq_len]\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "decoded_in = tokenizer.batch_decode(token_ids_in, skip_special_tokens=True)\n",
    "decoded_out = tokenizer.batch_decode(token_ids_out, skip_special_tokens=True)\n",
    "# print(decoded_in)\n",
    "# print(decoded_out)\n",
    "\n",
    "from itertools import zip_longest\n",
    "\n",
    "for in_str, out_str in zip(decoded_in, decoded_out):\n",
    "    in_words = in_str.split()\n",
    "    out_words = out_str.split()\n",
    "    for in_word, out_word in zip_longest(in_words, out_words, fillvalue=\"\"):\n",
    "        print(f\"{in_word:<15} | {out_word}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daae6c2-a912-4e1f-9da3-d5fa09645c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f874861-93ee-4838-9e62-97d5270889e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f582c1f4-f817-45c0-9327-56b93b213020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 39\n"
     ]
    }
   ],
   "source": [
    "print(len(optim_hp_list), len(all_mh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08373c1c-b5ad-4dbd-ac78-5024bf6e9bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD, train_accuracy:0.9998698234558105, test_accuracy:0.5381667017936707, lse:60.333333333333336\n",
      "SGD-SAM, train_accuracy:0.9996744990348816, test_accuracy:0.5221666693687439, lse:62.0\n",
      "ADAM, train_accuracy:0.9994141459465027, test_accuracy:0.5755000114440918, lse:1326.3333333333333\n",
      "ADAM-SAM-R0, train_accuracy:0.9992188215255737, test_accuracy:0.5898333787918091, lse:1417.3333333333333\n",
      "ADAM-SAM, train_accuracy:0.9994140863418579, test_accuracy:0.5566667318344116, lse:1307.0\n",
      "ADAM-UB-1e0, train_accuracy:0.9991536140441895, test_accuracy:0.5726667642593384, lse:1045.6666666666667\n",
      "ADAM-UB-1e0-SAM, train_accuracy:0.9995443224906921, test_accuracy:0.5693333745002747, lse:1139.0\n",
      "ADAM-UB-5e-1, train_accuracy:0.9994140863418579, test_accuracy:0.5721666812896729, lse:1001.0\n",
      "ADAM-UB-1e-1, train_accuracy:0.9996744990348816, test_accuracy:0.5730000734329224, lse:1139.6666666666667\n",
      "ADAM-UB-5e-2, train_accuracy:0.9992188215255737, test_accuracy:0.561333417892456, lse:889.0\n",
      "ADAM-UB-1e-2, train_accuracy:0.9994141459465027, test_accuracy:0.5073333978652954, lse:624.6666666666666\n",
      "ADAM-UB-5e-3, train_accuracy:0.7018880248069763, test_accuracy:0.3358333706855774, lse:357.6666666666667\n",
      "ADAM-UB-1e-3, train_accuracy:0.1070963591337204, test_accuracy:0.10200001299381256, lse:100.0\n"
     ]
    }
   ],
   "source": [
    "optim_names = ['SGD', 'SGD-SAM', 'ADAM', 'ADAM-SAM-R0', 'ADAM-SAM', 'ADAM-UB-1e0', 'ADAM-UB-1e0-SAM', 'ADAM-UB-5e-1','ADAM-UB-1e-1', 'ADAM-UB-5e-2', 'ADAM-UB-1e-2', 'ADAM-UB-5e-3','ADAM-UB-1e-3']\n",
    "stat_names = ['train_accuracy', 'test_accuracy', 'lse']\n",
    "for i in range(len(optim_hp_list)):\n",
    "    stats = [ 0 for j in range(len(stat_names))]\n",
    "    for j in range(len(stats)):\n",
    "        for k in range(3*i, 3*i+3):\n",
    "            stats[j] += all_mh[k][stat_names[j]][-1]\n",
    "    out = f\"{optim_names[i]}\"\n",
    "    for j in range(len(stats)):\n",
    "        out += f\", {stat_names[j]}:{stats[j]/3}\"\n",
    "    print(out)\n",
    "    # print(optim_hp_list[i], np.mean(all_mh[3*i:3*i+3]['train_accuracy'][-1]), np.mean(all_mh[3*i:3*i+3]['test_accuracy'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d0fcc8-c67d-4817-baa5-f39b980fa980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(all_mh)):\n",
    "    print(hyp_list[i][0], all_mh[i]['train_accuracy'][-1], all_mh[i]['test_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb7171db-c859-483f-b386-fbdd7084cb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1, 0.0, 0.0, 0.0, False, 0.0, 1) 0.99921876 0.508\n",
      "(0.1, 0.0, 0.0, 0.0, True, 0.1, 1) 0.99902344 0.5245\n",
      "(0.005, 0.9, 0.99, 0.0, False, 0.0, 1) 0.99921876 0.5705\n",
      "(0.005, 0.9, 0.99, 0.0, True, 0.0, 1) 0.99902344 0.583\n",
      "(0.005, 0.9, 0.99, 0.0, True, 0.001, 1) 0.99902344 0.57750005\n",
      "(0.005, 0.9, 0.99, -1.0, False, 0.0, 1) 0.9996094 0.586\n",
      "(0.005, 0.9, 0.99, -1.0, True, 0.001, 1) 0.9996094 0.56450003\n",
      "(0.005, 0.9, 0.99, -0.5, False, 0.0, 1) 0.9996094 0.5755\n",
      "(0.005, 0.9, 0.99, -0.1, False, 0.0, 1) 0.9998047 0.573\n",
      "(0.005, 0.9, 0.99, -0.05, False, 0.0, 1) 0.9994141 0.58100003\n",
      "(0.005, 0.9, 0.99, -0.01, False, 0.0, 1) 0.99902344 0.54700005\n",
      "(0.005, 0.9, 0.99, -0.005, False, 0.0, 1) 0.9142578 0.3535\n",
      "(0.005, 0.9, 0.99, -0.001, False, 0.0, 1) 0.09980469 0.108500004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(all_mh)):\n",
    "    print(hyp_list[i][0], all_mh[i]['train_accuracy'][-1], all_mh[i]['test_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b06a76-3cca-4ade-8291-16fcf5250747",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
