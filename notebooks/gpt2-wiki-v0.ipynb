{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96934865-c7fe-434c-b742-641e91d10e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved up\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "print(\"Moved up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58afea8-beda-4db5-8c9e-68237dff61e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 21:37:45.461095: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746045465.475055  351374 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746045465.479436  351374 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_MEM_FRACTION=.9\n",
      "devices [CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import copy\n",
    "import pickle\n",
    "import typing as tp\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.flatten_util as fu\n",
    "from flax import linen as nn  # Linen API\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import lib_data\n",
    "import utils\n",
    "import modules\n",
    "import callbacks\n",
    "\n",
    "%env XLA_PYTHON_CLIENT_MEM_FRACTION=.9\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print(\"devices\", jax.devices())\n",
    "\n",
    "shade_colours = plt.get_cmap('Set3')\n",
    "dark_colours = plt.get_cmap('tab10')\n",
    "all_colours = plt.get_cmap('tab20')\n",
    "\n",
    "def light_colours(i):\n",
    "    return all_colours(2*i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3def92-3cc5-44cd-b12c-c7514224659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env XLA_FLAGS=--xla_gpu_deterministic_ops=true\n",
    "# %env XLA_FLAGS=--xla_gpu_deterministic_reductions=true\n",
    "%env XLA_FLAGS=--xla_gpu_deterministic_ops=true --xla_gpu_deterministic_reductions=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e68954-02f0-4f13-bc71-8fc7b3472765",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3e3216-8f97-4f75-8ed5-db3f36752aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized dataset from disk...\n",
      "Building LM datasets...\n",
      "Flattening token sequences...\n",
      "Total 2391884 tokens. Creating 18686 chunks with stride 128\n",
      "Flattening token sequences...\n",
      "Total 283287 tokens. Creating 1000 chunks with stride 128\n",
      "Train: 18686  Eval: 1000  Hess: 18686\n",
      "Input shape: (127,) Target shape: (127,)\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "n_out = 1\n",
    "\n",
    "n_train: int = 20000\n",
    "n_eval: int = 1000\n",
    "n_hess: int = 10\n",
    "\n",
    "importlib.reload(lib_data)\n",
    "def __get_datasets():\n",
    "    datasets = lib_data.get_wikitext2_dataset(block_size=128, max_train_samples=n_train, max_eval_samples=n_eval)\n",
    "\n",
    "    print(\"Train:\", len(datasets[0]), \" Eval:\", len(datasets[1]), \" Hess:\", len(datasets[2]))\n",
    "    x, y = datasets[0][0]\n",
    "    print(\"Input shape:\", x.shape, \"Target shape:\", y.shape)\n",
    "\n",
    "    data_name = \"wiki2_\"+str(n_out)+\"cl_\"+str(n_train) + \"_\" + str(n_eval) \n",
    "    # print(len(datasets))\n",
    "    return datasets, data_name\n",
    "\n",
    "datasets, data_name = __get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1825cf-cf66-4628-b1df-0fd549e6b388",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "847ce83c-7ef5-4b66-90ea-57ea34b43343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "importlib.reload(modules)\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    config: ConfigDict\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        features = x.shape[-1]\n",
    "        x = nn.LayerNorm(dtype=self.config.dtype)(x)\n",
    "        x = nn.Dense(self.config.mlp_expansion * features, dtype=self.config.dtype)(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = nn.Dense(features, dtype=self.config.dtype)(x)\n",
    "        x = nn.Dropout(rate=self.config.dropout_rate)(x, deterministic=not self.train)\n",
    "        return x\n",
    "\n",
    "def dot_product_attention(query, key, value, mask, softmax_dtype=jnp.float32):\n",
    "    scale = query.shape[-1] ** -0.5\n",
    "    query = query.astype(softmax_dtype) * scale\n",
    "    key = key.astype(softmax_dtype)\n",
    "    weights = jnp.einsum(\"...qhd,...khd->...hqk\", query, key)\n",
    "    if mask is not None:\n",
    "        weights = jnp.where(mask, weights, jnp.finfo(softmax_dtype).min)\n",
    "    weights = nn.softmax(weights, axis=-1).astype(query.dtype)\n",
    "    return jnp.einsum(\"...hqk,...khd->...qhd\", weights, value)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    config: ConfigDict\n",
    "    mask: tp.Optional[jax.Array]\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        features = x.shape[-1]\n",
    "        x = nn.LayerNorm(dtype=self.config.dtype)(x)\n",
    "        qkv = nn.DenseGeneral(\n",
    "            features=(self.config.num_heads, self.config.head_dim * 3),\n",
    "            axis=-1, dtype=self.config.dtype\n",
    "        )(x)\n",
    "        q, k, v = jnp.split(qkv, 3, axis=-1)\n",
    "        attn = dot_product_attention(q, k, v, mask=self.mask, softmax_dtype=self.config.softmax_dtype)\n",
    "        x = nn.DenseGeneral(features=features, axis=(-2, -1), dtype=self.config.dtype)(attn)\n",
    "        x = nn.Dropout(rate=self.config.dropout_rate)(x, deterministic=not self.train)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    config: ConfigDict\n",
    "    mask: tp.Optional[jax.Array]\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        mlp = MLPBlock\n",
    "        if \"MLP\" in self.config.remat:\n",
    "            mlp = nn.remat(mlp, prevent_cse=False)\n",
    "        attn = AttentionBlock\n",
    "        if \"Attn\" in self.config.remat:\n",
    "            attn = nn.remat(attn, prevent_cse=False)\n",
    "\n",
    "        x = x + attn(config=self.config, mask=self.mask, train=self.train)(x)\n",
    "        x = x + mlp(config=self.config, train=self.train)(x)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    config: ConfigDict\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask=None, train=True):\n",
    "        if mask is None and self.config.causal_mask:\n",
    "            mask = nn.make_causal_mask(x, dtype=jnp.bool_)\n",
    "\n",
    "        x = nn.Embed(self.config.vocab_size, self.config.hidden_size, dtype=self.config.dtype)(x)\n",
    "        pos_emb = self.param(\"pos_emb\", nn.initializers.normal(0.02),\n",
    "                             (self.config.max_seq_len, self.config.hidden_size)).astype(self.config.dtype)\n",
    "        x += pos_emb[None, :x.shape[1]]\n",
    "\n",
    "        block_fn = functools.partial(TransformerBlock, config=self.config, mask=mask, train=train)\n",
    "\n",
    "        if self.config.scan_layers:\n",
    "            block = block_fn(name=\"block\")\n",
    "            x, _ = nn.scan(\n",
    "                lambda module, carry, _: (module(carry), None),\n",
    "                variable_axes={\"params\": 0},\n",
    "                split_rngs={\"params\": True, \"dropout\": True},\n",
    "                length=self.config.num_layers\n",
    "            )(block, x, ())\n",
    "        else:\n",
    "            for i in range(self.config.num_layers):\n",
    "                x = block_fn(name=f\"block_{i}\")(x)\n",
    "\n",
    "        x = nn.LayerNorm(dtype=self.config.dtype)(x)\n",
    "        x = nn.Dense(self.config.num_outputs, dtype=self.config.dtype)(x)\n",
    "        return x.astype(jnp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "807718aa-21fe-4d29-a2ca-b931057d684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer_L4_H256_Heads4\n"
     ]
    }
   ],
   "source": [
    "def __get_arch__():\n",
    "\n",
    "    config = ConfigDict()\n",
    "    config.vocab_size = 50257\n",
    "    config.hidden_size = 256\n",
    "    config.num_layers = 4\n",
    "    config.num_heads = 4\n",
    "    config.head_dim = 64\n",
    "    config.mlp_expansion = 4\n",
    "    config.dropout_rate = 0.1\n",
    "    config.max_seq_len = 512\n",
    "    config.num_outputs = 50257\n",
    "    config.dtype = jnp.float32\n",
    "    config.causal_mask = True\n",
    "    config.softmax_dtype = jnp.float32\n",
    "    config.remat = [\"MLP\", \"Attn\"]\n",
    "    config.scan_layers = False\n",
    "\n",
    "    model = Transformer(config)\n",
    "    model_name = f\"Transformer_L{config.num_layers}_H{config.hidden_size}_Heads{config.num_heads}\"\n",
    "    return model, model_name\n",
    "\n",
    "model_arch, model_name = __get_arch__()\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d222c-9284-48b2-ba72-6ae1cf02ef3d",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f871d6d-d289-4915-8975-f9c44d058baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "importlib.reload(modules)\n",
    "from optax import contrib\n",
    "\n",
    "def __get_optim__(warmup_steps, lr, b1, b2, b3, option=\"\", rho=None, sync_period=1):\n",
    "    # warmup_steps, lr, b1, b2, b3 = hyps['warmup_steps'], hyps['lr'], hyps['b1'], hyps['b2'], hyps['b3']\n",
    "    if option == 'sam':\n",
    "        assert rho is not None\n",
    "        warmup_scheduler = optax.linear_schedule(init_value=0.0, end_value=lr,\n",
    "                                                transition_steps=warmup_steps,\n",
    "                                                transition_begin=0,)\n",
    "        constant_scheduler = optax.constant_schedule(lr)\n",
    "        lr_scheduler = optax.join_schedules([warmup_scheduler, constant_scheduler], boundaries=[warmup_steps])\n",
    "        base_opt = modules.get_sgd_optimizer(lr_scheduler, b1, b2, b3, verbose=False)\n",
    "        adv_opt = modules.get_sgd_optimizer(rho, b1, b2, b3, verbose=False)\n",
    "        optimizer = contrib.sam(base_opt, adv_opt, sync_period=sync_period, opaque_mode=True) # sam opt\n",
    "        optim_name = f\"sgdFam-SAM_1b{b1}_2b{b2}_3b{b3}_lr{lr}_warmup{warmup_steps}_rho{rho}_syncT{sync_period}\"\n",
    "\n",
    "    else:\n",
    "        warmup_scheduler = optax.linear_schedule(init_value=0.0, end_value=lr,\n",
    "                                                transition_steps=warmup_steps,\n",
    "                                                transition_begin=0,)\n",
    "        constant_scheduler = optax.constant_schedule(lr)\n",
    "        lr_scheduler = optax.join_schedules([warmup_scheduler, constant_scheduler], boundaries=[warmup_steps])\n",
    "        optimizer = modules.get_sgd_optimizer(lr_scheduler, b1, b2, b3, verbose=False)\n",
    "        optim_name = f\"sgdFam_1b{b1}_2b{b2}_3b{b3}_lr{lr}_warmup{warmup_steps}\"\n",
    "        \n",
    "    return optimizer, optim_name\n",
    "\n",
    "optimizer, optim_name = __get_optim__(2, 0.1, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae0321-a5a3-4190-9934-f87074466230",
   "metadata": {},
   "source": [
    "# Model Params (Fixed and Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bba4401-c995-444b-8b5a-fcf6efdb42e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "import itertools\n",
    "### FIXED\n",
    "warmup_steps = 2\n",
    "bs = 32\n",
    "eval_bs = 32\n",
    "n_epochs = 2000\n",
    "loss_fn = optax.softmax_cross_entropy_with_integer_labels\n",
    "\n",
    "# FLEXIBLE\n",
    "# lr = 0.005\n",
    "# beta_list = [(0., 0., 0.), (0., 0.99, 0.)]\n",
    "# optim_hp = LR, B1, B2, B3, SAM, Rho, sync_period\n",
    "optim_hp_list = [\n",
    "    (5e-3, 0.9, 0.99, 0., False, 0., 1), \n",
    "]\n",
    "seed_list = [x for x in range(1)]\n",
    "# sam_list = [False, True]\n",
    "s = [optim_hp_list, seed_list]\n",
    "hyp_list = list(itertools.product(*s))\n",
    "print(len(hyp_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a3fe1b-9c7b-4df5-95c2-74d36952280c",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f047b6bf-6631-4765-b580-5729e89aa0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "sws = 5\n",
    "cb_freq = 1\n",
    "hess_freq = int(1e8) # really large\n",
    "importlib.reload(callbacks)\n",
    "def __get_cbs__(state, compute_hessian=False):\n",
    "    cbs = []\n",
    "    cbs.append(callbacks.saveWeightsCB(sws, grad=True))\n",
    "    # cbs.append(callbacks.thinCB(thin_freq=cb_freq))\n",
    "\n",
    "    if compute_hessian:\n",
    "        hvpCB = callbacks.hvpCB(loss_fn=loss_fn, batches=(datasets[2].data[:n_hess], datasets[2].targets[:n_hess]), \n",
    "                            save_freq=hess_freq, hess_bs=n_hess, state=state, bn=False)\n",
    "        cbs.append(hvpCB)   \n",
    "        specCB = callbacks.spectrumCB(n_eigs=20, n_evecs=10, \n",
    "                    loss_fn=loss_fn, seed=seed, hvpCB=hvpCB, save_freq=hess_freq, verbose=False)\n",
    "        cbs.append(specCB)\n",
    "\n",
    "        esCB = callbacks.earlyStopCB(acc_threshold=0.999, cbs=None, min_eps=sws, max_eps=n_epochs,conseq_eps=3,\n",
    "                                 final_cbs=[hvpCB, specCB], verbose=False, low_eps=max(sws, 100), low_thresh=0.11, )\n",
    "    else:\n",
    "        esCB = callbacks.earlyStopCB(acc_threshold=0.999, cbs=None, min_eps=sws, max_eps=n_epochs, conseq_eps=5,\n",
    "                                 verbose=False, low_eps=max(sws, 100), low_thresh=0., )\n",
    "    cbs.append(esCB)\n",
    "    return cbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62ae84-b4c1-4432-b483-740b815ac760",
   "metadata": {},
   "source": [
    "# Train State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c25ef97f-b46a-4b75-9da9-93f126878a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "import jax.numpy as jnp\n",
    "from clu.metrics import Metric\n",
    "import optax\n",
    "\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class Perplexity(Metric):\n",
    "  \"\"\"Computes perplexity from logits and integer labels.\n",
    "\n",
    "  This assumes logits have shape [..., vocab_size] and labels have shape [...].\n",
    "  \"\"\"\n",
    "\n",
    "  total_log_likelihood: jnp.ndarray\n",
    "  total_tokens: jnp.ndarray\n",
    "\n",
    "  @classmethod\n",
    "  def empty(cls) -> Perplexity:\n",
    "    return cls(\n",
    "        total_log_likelihood=jnp.array(0.0, dtype=jnp.float32),\n",
    "        total_tokens=jnp.array(0, dtype=jnp.int32)\n",
    "    )\n",
    "\n",
    "  @classmethod\n",
    "  def from_model_output(\n",
    "      cls, *, logits: jnp.ndarray, labels: jnp.ndarray, **kwargs\n",
    "  ) -> Perplexity:\n",
    "    if logits.ndim != labels.ndim + 1 or labels.dtype != jnp.int32:\n",
    "      raise ValueError(\n",
    "          f\"Expected labels.dtype==jnp.int32 and logits.ndim={logits.ndim}==\"\n",
    "          f\"labels.ndim+1={labels.ndim + 1}\"\n",
    "      )\n",
    "\n",
    "    # Flatten for token-level cross entropy\n",
    "    vocab_size = logits.shape[-1]\n",
    "    logits_flat = logits.reshape(-1, vocab_size)\n",
    "    labels_flat = labels.reshape(-1)\n",
    "\n",
    "    # Negative log-likelihood per token\n",
    "    nll = optax.softmax_cross_entropy_with_integer_labels(logits_flat, labels_flat)\n",
    "\n",
    "    return cls(\n",
    "        total_log_likelihood=jnp.sum(nll),\n",
    "        total_tokens=labels_flat.size\n",
    "    )\n",
    "\n",
    "  def merge(self, other: Perplexity) -> Perplexity:\n",
    "    return Perplexity(\n",
    "        total_log_likelihood=self.total_log_likelihood + other.total_log_likelihood,\n",
    "        total_tokens=self.total_tokens + other.total_tokens\n",
    "    )\n",
    "\n",
    "  def compute(self) -> jnp.ndarray:\n",
    "    avg_nll = self.total_log_likelihood / self.total_tokens\n",
    "    return jnp.exp(avg_nll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10a0d986-e610-48b1-8b2b-837cd218d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import struct                # Flax dataclasses\n",
    "from clu import metrics\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "importlib.reload(modules)\n",
    "\n",
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    accuracy: metrics.Accuracy\n",
    "    perplexity: Perplexity\n",
    "    loss: metrics.Average.from_output('loss')\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    rng: jax.Array\n",
    "\n",
    "class TrainStateBN(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    batch_stats: tp.Any\n",
    "    rng: jax.Array\n",
    "\n",
    "class TrainStateSAM(modules.TrainStateSAM):\n",
    "    metrics: Metrics\n",
    "    batch_stats: tp.Any\n",
    "    rng: jax.Array\n",
    "\n",
    "def create_train_state(model, optimizer, inputs, rng, option=\"\"):\n",
    "    \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "    rng, model_rng = jax.random.split(rng)\n",
    "    if option == \"\":\n",
    "        params = model.init(model_rng, jnp.ones_like(inputs[0][jnp.newaxis, :]))['params'] # initialize parameters by passing a template image\n",
    "        \n",
    "        tx = optimizer\n",
    "        return TrainState.create(\n",
    "          apply_fn=model.apply, params=params, tx=tx, metrics=Metrics.empty(), rng=rng)\n",
    "        \n",
    "    elif option == \"bn\":\n",
    "        variables = model.init(model_rng, jnp.ones_like(inputs[0][jnp.newaxis, :])) # initialize parameters by passing a template image\n",
    "        params = variables['params']\n",
    "        batch_stats = variables['batch_stats']\n",
    "        \n",
    "        tx = optimizer\n",
    "        return TrainStateBN.create(\n",
    "          apply_fn=model.apply, params=params, tx=tx, batch_stats=batch_stats, \n",
    "          metrics=Metrics.empty(), rng=rng)\n",
    "        \n",
    "    elif option == \"sam\":\n",
    "        variables = model.init(model_rng, jnp.ones_like(inputs[0][jnp.newaxis, :])) # initialize parameters by passing a template image\n",
    "        params = variables['params']\n",
    "        batch_stats = variables['batch_stats']\n",
    "        \n",
    "        tx = optimizer\n",
    "        return TrainStateSAM.create(\n",
    "          apply_fn=model.apply, params=params, tx=tx, batch_stats=batch_stats, \n",
    "          metrics=Metrics.empty(), rng=rng)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98af9c-1eaf-4282-a98f-a6e2a910dffe",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f060f232-f47d-4721-92a0-c6309ff6f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eb9178-e47b-45cf-ada0-bbbc84bc7342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "import training\n",
    "importlib.reload(training)\n",
    "\n",
    "load_files = False\n",
    "compute_hessian = False\n",
    "force_train = True\n",
    "\n",
    "all_mh = []\n",
    "all_exp_names = []\n",
    "\n",
    "for hyp in hyp_list:\n",
    "    \n",
    "    metrics_history = {'train_loss': [],\n",
    "                   'train_accuracy': [],\n",
    "                   'train_perplexity': [],\n",
    "                   'test_loss': [],\n",
    "                   'test_accuracy': [],\n",
    "                   'test_perplexity': [],\n",
    "                      }\n",
    "\n",
    "    lr, b1, b2, b3, sam, sam_rho, sync_T = hyp[0]\n",
    "    seed = hyp[1]\n",
    "    option = 'sam' if sam else \"\"\n",
    "    \n",
    "    if datasets is None:\n",
    "        datasets, data_name = __get_datasets__()\n",
    "    \n",
    "    train_loader = lib_data.NumpyLoader(datasets[0], batch_size=bs, shuffle=True)\n",
    "    for sample_batch in train_loader:\n",
    "        break\n",
    "    \n",
    "    test_loader = lib_data.NumpyLoader(datasets[1], batch_size=eval_bs)\n",
    "    dataloaders = [train_loader, test_loader]\n",
    "    \n",
    "    model, model_name = __get_arch__()\n",
    "    model_name += \"_seed\"+str(seed)\n",
    "\n",
    "    optim, optim_name = __get_optim__(warmup_steps, lr, b1, b2, b3, option=option, rho=sam_rho, sync_period=sync_T)\n",
    "    optim_name += f\"_epochs{n_epochs}_bs{bs}\"\n",
    "\n",
    "    init_rng = jax.random.PRNGKey(seed)\n",
    "    state = create_train_state(model, optim, sample_batch[0], init_rng, option=option)\n",
    "    del init_rng  # Must not be used anymore.\n",
    "    \n",
    "    cbs = __get_cbs__(state, compute_hessian=compute_hessian)\n",
    "    cb_name_str = utils.get_callback_name_str(cbs)\n",
    "    cb_name_list = utils.get_callback_name_list(cbs)\n",
    "    # break\n",
    "    num_params = utils.count_params(state.params)\n",
    "    print(\"num params\", num_params)\n",
    "\n",
    "    experiment_name = utils.get_now() + \"_\" + data_name + \"_\" + model_name + \"_\" + optim_name\n",
    "    \n",
    "    try:\n",
    "        if force_train:\n",
    "            raise FileNotFoundError\n",
    "        experiment_name, lse = utils.find_latest_exp(experiment_name, n_epochs, save_freq=cb_freq, \n",
    "                                                   cbs=cb_name_list, unknown_lse=True, verbose=False)\n",
    "        metrics_history = utils.load_thing(\"traj/\" + experiment_name + \"/metrics.pkl\")\n",
    "        print(f\"tr_acc: {metrics_history['train_accuracy'][-1]:0%}, te_acc: {metrics_history['test_accuracy'][-1]:0%}\")\n",
    "        metrics_history['lse'] = [lse]\n",
    "        if compute_hessian:\n",
    "            eigvals = utils.load_thing(\"traj/\" + experiment_name + \"/eigvals.pkl\")\n",
    "            metrics_history['eigvals'] = eigvals\n",
    "            print(f\"sharp: {metrics_history['eigvals'][-1][0]}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        metrics_history = training.train_model(state, model, loss_fn, metrics_history, n_epochs, dataloaders, \\\n",
    "                                                   experiment_name, cbs, option=option, force_fb=False, tqdm_over_epochs=1, \n",
    "                                              eval_freq=1, gradient_accumulation=16)         \n",
    "        \n",
    "    all_mh.append(metrics_history)\n",
    "    all_exp_names.append(experiment_name)\n",
    "    \n",
    "    print(experiment_name, \"complete\")\n",
    "    print(\"\\n ---------------------------------------------------------------------------------------------------------\\n\")\n",
    "# Training: datasets, hps, arch_func, optim_func, cb_func, -> train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daae6c2-a912-4e1f-9da3-d5fa09645c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f874861-93ee-4838-9e62-97d5270889e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f582c1f4-f817-45c0-9327-56b93b213020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 39\n"
     ]
    }
   ],
   "source": [
    "print(len(optim_hp_list), len(all_mh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08373c1c-b5ad-4dbd-ac78-5024bf6e9bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD, train_accuracy:0.9998698234558105, test_accuracy:0.5381667017936707, lse:60.333333333333336\n",
      "SGD-SAM, train_accuracy:0.9996744990348816, test_accuracy:0.5221666693687439, lse:62.0\n",
      "ADAM, train_accuracy:0.9994141459465027, test_accuracy:0.5755000114440918, lse:1326.3333333333333\n",
      "ADAM-SAM-R0, train_accuracy:0.9992188215255737, test_accuracy:0.5898333787918091, lse:1417.3333333333333\n",
      "ADAM-SAM, train_accuracy:0.9994140863418579, test_accuracy:0.5566667318344116, lse:1307.0\n",
      "ADAM-UB-1e0, train_accuracy:0.9991536140441895, test_accuracy:0.5726667642593384, lse:1045.6666666666667\n",
      "ADAM-UB-1e0-SAM, train_accuracy:0.9995443224906921, test_accuracy:0.5693333745002747, lse:1139.0\n",
      "ADAM-UB-5e-1, train_accuracy:0.9994140863418579, test_accuracy:0.5721666812896729, lse:1001.0\n",
      "ADAM-UB-1e-1, train_accuracy:0.9996744990348816, test_accuracy:0.5730000734329224, lse:1139.6666666666667\n",
      "ADAM-UB-5e-2, train_accuracy:0.9992188215255737, test_accuracy:0.561333417892456, lse:889.0\n",
      "ADAM-UB-1e-2, train_accuracy:0.9994141459465027, test_accuracy:0.5073333978652954, lse:624.6666666666666\n",
      "ADAM-UB-5e-3, train_accuracy:0.7018880248069763, test_accuracy:0.3358333706855774, lse:357.6666666666667\n",
      "ADAM-UB-1e-3, train_accuracy:0.1070963591337204, test_accuracy:0.10200001299381256, lse:100.0\n"
     ]
    }
   ],
   "source": [
    "optim_names = ['SGD', 'SGD-SAM', 'ADAM', 'ADAM-SAM-R0', 'ADAM-SAM', 'ADAM-UB-1e0', 'ADAM-UB-1e0-SAM', 'ADAM-UB-5e-1','ADAM-UB-1e-1', 'ADAM-UB-5e-2', 'ADAM-UB-1e-2', 'ADAM-UB-5e-3','ADAM-UB-1e-3']\n",
    "stat_names = ['train_accuracy', 'test_accuracy', 'lse']\n",
    "for i in range(len(optim_hp_list)):\n",
    "    stats = [ 0 for j in range(len(stat_names))]\n",
    "    for j in range(len(stats)):\n",
    "        for k in range(3*i, 3*i+3):\n",
    "            stats[j] += all_mh[k][stat_names[j]][-1]\n",
    "    out = f\"{optim_names[i]}\"\n",
    "    for j in range(len(stats)):\n",
    "        out += f\", {stat_names[j]}:{stats[j]/3}\"\n",
    "    print(out)\n",
    "    # print(optim_hp_list[i], np.mean(all_mh[3*i:3*i+3]['train_accuracy'][-1]), np.mean(all_mh[3*i:3*i+3]['test_accuracy'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d0fcc8-c67d-4817-baa5-f39b980fa980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(all_mh)):\n",
    "    print(hyp_list[i][0], all_mh[i]['train_accuracy'][-1], all_mh[i]['test_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb7171db-c859-483f-b386-fbdd7084cb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1, 0.0, 0.0, 0.0, False, 0.0, 1) 0.99921876 0.508\n",
      "(0.1, 0.0, 0.0, 0.0, True, 0.1, 1) 0.99902344 0.5245\n",
      "(0.005, 0.9, 0.99, 0.0, False, 0.0, 1) 0.99921876 0.5705\n",
      "(0.005, 0.9, 0.99, 0.0, True, 0.0, 1) 0.99902344 0.583\n",
      "(0.005, 0.9, 0.99, 0.0, True, 0.001, 1) 0.99902344 0.57750005\n",
      "(0.005, 0.9, 0.99, -1.0, False, 0.0, 1) 0.9996094 0.586\n",
      "(0.005, 0.9, 0.99, -1.0, True, 0.001, 1) 0.9996094 0.56450003\n",
      "(0.005, 0.9, 0.99, -0.5, False, 0.0, 1) 0.9996094 0.5755\n",
      "(0.005, 0.9, 0.99, -0.1, False, 0.0, 1) 0.9998047 0.573\n",
      "(0.005, 0.9, 0.99, -0.05, False, 0.0, 1) 0.9994141 0.58100003\n",
      "(0.005, 0.9, 0.99, -0.01, False, 0.0, 1) 0.99902344 0.54700005\n",
      "(0.005, 0.9, 0.99, -0.005, False, 0.0, 1) 0.9142578 0.3535\n",
      "(0.005, 0.9, 0.99, -0.001, False, 0.0, 1) 0.09980469 0.108500004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(all_mh)):\n",
    "    print(hyp_list[i][0], all_mh[i]['train_accuracy'][-1], all_mh[i]['test_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b06a76-3cca-4ade-8291-16fcf5250747",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
