{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96934865-c7fe-434c-b742-641e91d10e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved up\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "print(\"Moved up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58afea8-beda-4db5-8c9e-68237dff61e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 11:21:01.903919: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746181261.917374 2402372 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746181261.921451 2402372 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devices [CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import copy\n",
    "import pickle\n",
    "import typing as tp\n",
    "from functools import partial\n",
    "\n",
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.flatten_util as fu\n",
    "from flax import linen as nn  # Linen API\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import lib_data\n",
    "import utils\n",
    "import modules\n",
    "import callbacks\n",
    "\n",
    "# %env XLA_PYTHON_CLIENT_MEM_FRACTION=.9\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print(\"devices\", jax.devices())\n",
    "\n",
    "shade_colours = plt.get_cmap('Set3')\n",
    "dark_colours = plt.get_cmap('tab10')\n",
    "all_colours = plt.get_cmap('tab20')\n",
    "\n",
    "def light_colours(i):\n",
    "    return all_colours(2*i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e68954-02f0-4f13-bc71-8fc7b3472765",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3e3216-8f97-4f75-8ed5-db3f36752aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized dataset from disk...\n",
      "Building LM datasets...\n",
      "Flattening token sequences...\n",
      "Total 2391884 tokens. Creating 2335 chunks with stride 1024\n",
      "Flattening token sequences...\n",
      "Total 283287 tokens. Creating 276 chunks with stride 1024\n",
      "Train: 2335  Eval: 276  Hess: 2335\n",
      "Input shape: (1023,) Target shape: (1023,)\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "n_out = 1\n",
    "\n",
    "n_train: int = 100000\n",
    "n_eval: int = 10000\n",
    "n_hess: int = 1\n",
    "\n",
    "importlib.reload(lib_data)\n",
    "def __get_datasets():\n",
    "    datasets = lib_data.get_wikitext2_dataset(block_size=1024, max_train_samples=n_train, max_eval_samples=n_eval)\n",
    "\n",
    "    print(\"Train:\", len(datasets[0]), \" Eval:\", len(datasets[1]), \" Hess:\", len(datasets[2]))\n",
    "    x, y = datasets[0][0]\n",
    "    print(\"Input shape:\", x.shape, \"Target shape:\", y.shape)\n",
    "\n",
    "    data_name = \"wiki2_\"+str(n_out)+\"cl_\"+str(n_train) + \"_\" + str(n_eval) \n",
    "    # print(len(datasets))\n",
    "    return datasets, data_name\n",
    "\n",
    "datasets, data_name = __get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1825cf-cf66-4628-b1df-0fd549e6b388",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847ce83c-7ef5-4b66-90ea-57ea34b43343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "importlib.reload(modules)\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    config: ConfigDict\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        features = x.shape[-1]\n",
    "        x = nn.LayerNorm(dtype=self.config.dtype)(x)\n",
    "        x = nn.Dense(self.config.mlp_expansion * features, dtype=self.config.dtype)(x)\n",
    "        x = nn.gelu(x, approximate=True)\n",
    "        x = nn.Dense(features, dtype=self.config.dtype)(x)\n",
    "        x = nn.Dropout(rate=self.config.dropout_rate)(x, deterministic=not self.train)\n",
    "        return x\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    config: ConfigDict\n",
    "    mask: tp.Optional[jax.Array]\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        features = x.shape[-1]\n",
    "        x = nn.LayerNorm(dtype=self.config.dtype)(x)\n",
    "        qkv = nn.DenseGeneral(\n",
    "            features=(self.config.num_heads, self.config.head_dim * 3),\n",
    "            axis=-1, dtype=self.config.dtype\n",
    "        )(x)\n",
    "        q, k, v = jnp.split(qkv, 3, axis=-1)\n",
    "\n",
    "        scale = q.shape[-1] ** -0.5\n",
    "        q = q.astype(self.config.softmax_dtype) * scale\n",
    "        k = k.astype(self.config.softmax_dtype)\n",
    "\n",
    "        q = q.transpose(0, 2, 1, 3) # [B T H D] to [B H T D]\n",
    "        k = k.transpose(0, 2, 1, 3) # [B T H D] to [B H T D]\n",
    "        v = v.transpose(0, 2, 1, 3) # [B T H D] to [B H T D]\n",
    "    \n",
    "        attn = q @ k.swapaxes(-2, -1) # [B H T D] @ [B H D T] -> [B H T T]\n",
    "    \n",
    "        if self.mask is not None:\n",
    "            attn = jnp.where(self.mask, attn, jnp.finfo(self.config.softmax_dtype).min)\n",
    "    \n",
    "        attn = nn.softmax(attn, axis=-1).astype(self.config.dtype)\n",
    "        attn = nn.Dropout(rate=self.config.dropout_rate)(attn, deterministic=not self.train)\n",
    "        y = attn @ v # [B H T T] @ [B H T D] -> [B H T D]\n",
    "        y = y.transpose(0, 2, 1, 3) # [B H T D] -> [B T H D]\n",
    "        y = y.reshape(x.shape)  # [B T H D] -> [B T C(H*D)]\n",
    "        y = nn.Dense(features, dtype=self.config.dtype)(y)\n",
    "        y = nn.Dropout(rate=self.config.dropout_rate)(y, deterministic=not self.train)\n",
    "        return y\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    config: ConfigDict\n",
    "    mask: tp.Optional[jax.Array]\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        mlp = MLPBlock\n",
    "        if \"MLP\" in self.config.remat:\n",
    "            mlp = nn.remat(mlp, prevent_cse=False)\n",
    "        attn = AttentionBlock\n",
    "        if \"Attn\" in self.config.remat:\n",
    "            attn = nn.remat(attn, prevent_cse=False)\n",
    "\n",
    "        x = x + attn(config=self.config, mask=self.mask, train=self.train)(x)\n",
    "        x = x + mlp(config=self.config, train=self.train)(x)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    config: ConfigDict\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask=None, train=True):\n",
    "        if mask is None and self.config.causal_mask:\n",
    "            mask = nn.make_causal_mask(x, dtype=jnp.bool_)\n",
    "\n",
    "        embed = nn.Embed(self.config.vocab_size, self.config.hidden_size, dtype=self.config.dtype, name='token_embed')\n",
    "        x = embed(x) \n",
    "        pos_emb = self.param(\"pos_emb\", nn.initializers.normal(0.02),\n",
    "                             (self.config.max_seq_len, self.config.hidden_size)).astype(self.config.dtype)\n",
    "        \n",
    "        x += pos_emb[None, :x.shape[1]]\n",
    "\n",
    "        block_fn = functools.partial(TransformerBlock, config=self.config, mask=mask, train=train)\n",
    "\n",
    "        if self.config.scan_layers:\n",
    "            block = block_fn(name=\"block\")\n",
    "            x, _ = nn.scan(\n",
    "                lambda module, carry, _: (module(carry), None),\n",
    "                variable_axes={\"params\": 0},\n",
    "                split_rngs={\"params\": True, \"dropout\": True},\n",
    "                length=self.config.num_layers\n",
    "            )(block, x, ())\n",
    "        else:\n",
    "            for i in range(self.config.num_layers):\n",
    "                x = block_fn(name=f\"block_{i}\")(x)\n",
    "\n",
    "        x = nn.LayerNorm(dtype=self.config.dtype)(x)\n",
    "\n",
    "        # weight tying\n",
    "        logits = x @ embed.embedding.T\n",
    "        return logits.astype(jnp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807718aa-21fe-4d29-a2ca-b931057d684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer_L12_H768_Heads12\n"
     ]
    }
   ],
   "source": [
    "config = ConfigDict()\n",
    "config.vocab_size = 50257\n",
    "config.hidden_size = 768\n",
    "config.num_layers = 12\n",
    "config.num_heads = 12\n",
    "config.head_dim = 64\n",
    "config.mlp_expansion = 4\n",
    "config.dropout_rate = 0.1\n",
    "config.max_seq_len = 1024\n",
    "config.num_outputs = 50257\n",
    "config.dtype = jnp.float32\n",
    "config.causal_mask = True\n",
    "config.softmax_dtype = jnp.float32\n",
    "config.remat = [\"MLP\", \"Attn\"]\n",
    "config.scan_layers = False\n",
    "\n",
    "def __get_arch__():\n",
    "\n",
    "    model = Transformer(config)\n",
    "    model_name = f\"Transformer_L{config.num_layers}_H{config.hidden_size}_Heads{config.num_heads}\"\n",
    "    return model, model_name\n",
    "\n",
    "model_arch, model_name = __get_arch__()\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d222c-9284-48b2-ba72-6ae1cf02ef3d",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f871d6d-d289-4915-8975-f9c44d058baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "importlib.reload(modules)\n",
    "from optax import contrib\n",
    "\n",
    "def __get_optim__(warmup_steps, lr, b1, b2, b3, option=\"\", rho=None, sync_period=1):\n",
    "    # warmup_steps, lr, b1, b2, b3 = hyps['warmup_steps'], hyps['lr'], hyps['b1'], hyps['b2'], hyps['b3']\n",
    "    if option == 'sam':\n",
    "        assert rho is not None\n",
    "        warmup_scheduler = optax.linear_schedule(init_value=0.0, end_value=lr,\n",
    "                                                transition_steps=warmup_steps,\n",
    "                                                transition_begin=0,)\n",
    "        constant_scheduler = optax.constant_schedule(lr)\n",
    "        lr_scheduler = optax.join_schedules([warmup_scheduler, constant_scheduler], boundaries=[warmup_steps])\n",
    "        base_opt = modules.get_sgd_optimizer(lr_scheduler, b1, b2, b3, verbose=False)\n",
    "        adv_opt = modules.get_sgd_optimizer(rho, b1, b2, b3, verbose=False)\n",
    "        optimizer = contrib.sam(base_opt, adv_opt, sync_period=sync_period, opaque_mode=True) # sam opt\n",
    "        optim_name = f\"sgdFam-SAM_1b{b1}_2b{b2}_3b{b3}_lr{lr}_warmup{warmup_steps}_rho{rho}_syncT{sync_period}\"\n",
    "\n",
    "    else:\n",
    "        warmup_scheduler = optax.linear_schedule(init_value=0.0, end_value=lr,\n",
    "                                                transition_steps=warmup_steps,\n",
    "                                                transition_begin=0,)\n",
    "        constant_scheduler = optax.constant_schedule(lr)\n",
    "        lr_scheduler = optax.join_schedules([warmup_scheduler, constant_scheduler], boundaries=[warmup_steps])\n",
    "        optimizer = modules.get_sgd_optimizer(lr_scheduler, b1, b2, b3, verbose=False)\n",
    "        optim_name = f\"sgdFam_1b{b1}_2b{b2}_3b{b3}_lr{lr}_warmup{warmup_steps}\"\n",
    "        \n",
    "    return optimizer, optim_name\n",
    "\n",
    "optimizer, optim_name = __get_optim__(2, 0.1, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae0321-a5a3-4190-9934-f87074466230",
   "metadata": {},
   "source": [
    "# Model Params (Fixed and Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bba4401-c995-444b-8b5a-fcf6efdb42e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "import itertools\n",
    "### FIXED\n",
    "warmup_steps = 2\n",
    "bs = 1\n",
    "eval_bs = 1\n",
    "n_epochs = 10\n",
    "loss_fn = optax.softmax_cross_entropy_with_integer_labels\n",
    "\n",
    "# FLEXIBLE\n",
    "# lr = 0.005\n",
    "# beta_list = [(0., 0., 0.), (0., 0.99, 0.)]\n",
    "# optim_hp = LR, B1, B2, B3, SAM, Rho, sync_period\n",
    "optim_hp_list = [\n",
    "    (5e-3, 0.9, 0.99, 0., False, 0., 1), \n",
    "]\n",
    "seed_list = [x for x in range(1)]\n",
    "# sam_list = [False, True]\n",
    "s = [optim_hp_list, seed_list]\n",
    "hyp_list = list(itertools.product(*s))\n",
    "print(len(hyp_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a3fe1b-9c7b-4df5-95c2-74d36952280c",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f047b6bf-6631-4765-b580-5729e89aa0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "sws = 5\n",
    "cb_freq = 1\n",
    "hess_freq = int(1e8) # really large\n",
    "importlib.reload(callbacks)\n",
    "def __get_cbs__(state, compute_hessian=False):\n",
    "    cbs = []\n",
    "    cbs.append(callbacks.saveWeightsCB(sws, grad=True))\n",
    "    # cbs.append(callbacks.thinCB(thin_freq=cb_freq))\n",
    "\n",
    "    if compute_hessian:\n",
    "        hvpCB = callbacks.hvpCB(loss_fn=loss_fn, batches=(datasets[2].data[:n_hess], datasets[2].targets[:n_hess]), \n",
    "                            save_freq=hess_freq, hess_bs=n_hess, state=state, bn=False)\n",
    "        cbs.append(hvpCB)   \n",
    "        specCB = callbacks.spectrumCB(n_eigs=20, n_evecs=10, \n",
    "                    loss_fn=loss_fn, seed=seed, hvpCB=hvpCB, save_freq=hess_freq, verbose=False)\n",
    "        cbs.append(specCB)\n",
    "\n",
    "        esCB = callbacks.earlyStopCB(acc_threshold=0.999, cbs=None, min_eps=sws, max_eps=n_epochs,conseq_eps=3,\n",
    "                                 final_cbs=[hvpCB, specCB], verbose=False, low_eps=max(sws, 100), low_thresh=0.11, )\n",
    "    else:\n",
    "        esCB = callbacks.earlyStopCB(acc_threshold=0.999, cbs=None, min_eps=sws, max_eps=n_epochs, conseq_eps=5,\n",
    "                                 verbose=False, low_eps=max(sws, 100), low_thresh=0., )\n",
    "    cbs.append(esCB)\n",
    "    return cbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62ae84-b4c1-4432-b483-740b815ac760",
   "metadata": {},
   "source": [
    "# Train State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a0d986-e610-48b1-8b2b-837cd218d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import struct                # Flax dataclasses\n",
    "from clu import metrics\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "from perplexity import Perplexity\n",
    "importlib.reload(modules)\n",
    "\n",
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    accuracy: metrics.Accuracy\n",
    "    perplexity: Perplexity\n",
    "    loss: metrics.Average.from_output('loss')\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    rng: jax.Array\n",
    "\n",
    "class TrainStateBN(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    batch_stats: tp.Any\n",
    "    rng: jax.Array\n",
    "\n",
    "class TrainStateSAM(modules.TrainStateSAM):\n",
    "    metrics: Metrics\n",
    "    batch_stats: tp.Any\n",
    "    rng: jax.Array\n",
    "\n",
    "def create_train_state(model, optimizer, inputs, rng, option=\"\"):\n",
    "    \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "    rng, model_rng = jax.random.split(rng)\n",
    "    if option == \"\":\n",
    "        params = model.init(model_rng, jnp.ones_like(inputs[0][jnp.newaxis, :]))['params'] # initialize parameters by passing a template image\n",
    "        \n",
    "        tx = optimizer\n",
    "        return TrainState.create(\n",
    "          apply_fn=model.apply, params=params, tx=tx, metrics=Metrics.empty(), rng=rng)\n",
    "        \n",
    "    elif option == \"bn\":\n",
    "        variables = model.init(model_rng, jnp.ones_like(inputs[0][jnp.newaxis, :])) # initialize parameters by passing a template image\n",
    "        params = variables['params']\n",
    "        batch_stats = variables['batch_stats']\n",
    "        \n",
    "        tx = optimizer\n",
    "        return TrainStateBN.create(\n",
    "          apply_fn=model.apply, params=params, tx=tx, batch_stats=batch_stats, \n",
    "          metrics=Metrics.empty(), rng=rng)\n",
    "        \n",
    "    elif option == \"sam\":\n",
    "        variables = model.init(model_rng, jnp.ones_like(inputs[0][jnp.newaxis, :])) # initialize parameters by passing a template image\n",
    "        params = variables['params']\n",
    "        batch_stats = variables['batch_stats']\n",
    "        \n",
    "        tx = optimizer\n",
    "        return TrainStateSAM.create(\n",
    "          apply_fn=model.apply, params=params, tx=tx, batch_stats=batch_stats, \n",
    "          metrics=Metrics.empty(), rng=rng)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98af9c-1eaf-4282-a98f-a6e2a910dffe",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f060f232-f47d-4721-92a0-c6309ff6f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "# from flax.linen import tabulate\n",
    "# tabulated_fn = tabulate(model, rngs={\"params\": jax.random.PRNGKey(0), \"dropout\": jax.random.PRNGKey(1)},\n",
    "#                            console_kwargs={\"width\": 200, \"force_jupyter\": False}) # Avoid output clipping in notebooks)\n",
    "\n",
    "# print(tabulated_fn(sample_batch[0], train=False))\n",
    "# # with open(\"gpt2_summary.txt\", \"w\") as f:\n",
    "#     # f.write(tab_fn(dummy_input, train=False))\n",
    "#     # f.write(tabulated_fn(sample_batch[0], train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d5f29b7-532c-4ef6-8b1f-001250dc47d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FlaxGPT2LMHeadModel, GPT2Tokenizer\n",
    "from flax.core import freeze, unfreeze\n",
    "\n",
    "def load_params(state):\n",
    "    hf_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\", dtype=jnp.float32)\n",
    "    hf_params = unfreeze(hf_model.params)\n",
    "    \n",
    "    params = state.params\n",
    "    params['token_embed']['embedding'] = hf_params['transformer']['wte']['embedding']\n",
    "    # Positional embedding\n",
    "    params['pos_emb'] = hf_params['transformer']['wpe']['embedding']\n",
    "    \n",
    "    # Loop through layers\n",
    "    for i in range(12):\n",
    "        hf_block = hf_params['transformer']['h'][str(i)]\n",
    "        block = params[f'block_{i}']\n",
    "        attn_key = \"CheckpointAttentionBlock_0\" if \"CheckpointAttentionBlock_0\" in block else \"AttentionBlock_0\"\n",
    "        mlp_key = \"CheckpointMLPBlock_0\" if \"CheckpointMLPBlock_0\" in block else \"MLPBlock_0\"\n",
    "    \n",
    "        # LayerNorms\n",
    "        block[attn_key]['LayerNorm_0']['scale'] = hf_block['ln_1']['scale']\n",
    "        block[attn_key]['LayerNorm_0']['bias'] = hf_block['ln_1']['bias']\n",
    "        block[mlp_key]['LayerNorm_0']['scale'] = hf_block['ln_2']['scale']\n",
    "        block[mlp_key]['LayerNorm_0']['bias'] = hf_block['ln_2']['bias']\n",
    "\n",
    "        #--- TRY 1\n",
    "        # # Attention QKV and output projection\n",
    "        # qkv_kernel = hf_block['attn']['c_attn']['kernel']  # shape [768, 3*768]\n",
    "        # qkv_bias = hf_block['attn']['c_attn']['bias']\n",
    "        # qkv_kernel = np.reshape(qkv_kernel, (config.hidden_size, config.num_heads, 3 * config.head_dim))\n",
    "        # q_kernel, k_kernel, v_kernel = np.split(qkv_kernel, 3, axis=-1)\n",
    "    \n",
    "        # qkv_bias = np.reshape(qkv_bias, (config.num_heads, 3 * config.head_dim))\n",
    "        # q_bias, k_bias, v_bias = np.split(qkv_bias, 3, axis=-1)\n",
    "    \n",
    "        # block[attn_key]['DenseGeneral_0']['kernel'] = np.concatenate([q_kernel, k_kernel, v_kernel], axis=-1)\n",
    "        # block[attn_key]['DenseGeneral_0']['bias'] = np.concatenate([q_bias, k_bias, v_bias], axis=-1)\n",
    "    \n",
    "        # block[attn_key]['Dense_0']['kernel'] = hf_block['attn']['c_proj']['kernel']\n",
    "        # block[attn_key]['Dense_0']['bias'] = hf_block['attn']['c_proj']['bias']\n",
    "    \n",
    "        # # MLP\n",
    "        # block[mlp_key]['Dense_0']['kernel'] = hf_block['mlp']['c_fc']['kernel']\n",
    "        # block[mlp_key]['Dense_0']['bias'] = hf_block['mlp']['c_fc']['bias']\n",
    "        # block[mlp_key]['Dense_1']['kernel'] = hf_block['mlp']['c_proj']['kernel']\n",
    "        # block[mlp_key]['Dense_1']['bias'] = hf_block['mlp']['c_proj']['bias']\n",
    "\n",
    "        #--- TRY 2\n",
    "\n",
    "        # # Attention: QKV projection (c_attn)\n",
    "        # block[attn_key]['DenseGeneral_0']['kernel'] = hf_block['attn']['c_attn']['kernel'].T\n",
    "        # block[attn_key]['DenseGeneral_0']['bias'] = hf_block['attn']['c_attn']['bias']\n",
    "\n",
    "        # # Attention output (c_proj)\n",
    "        # block[attn_key]['Dense_0']['kernel'] = hf_block['attn']['c_proj']['kernel'].T\n",
    "        # block[attn_key]['Dense_0']['bias'] = hf_block['attn']['c_proj']['bias']\n",
    "\n",
    "        # # MLP\n",
    "        # block[mlp_key]['Dense_0']['kernel'] = hf_block['mlp']['c_fc']['kernel'].T\n",
    "        # block[mlp_key]['Dense_0']['bias'] = hf_block['mlp']['c_fc']['bias']\n",
    "        # block[mlp_key]['Dense_1']['kernel'] = hf_block['mlp']['c_proj']['kernel'].T\n",
    "        # block[mlp_key]['Dense_1']['bias'] = hf_block['mlp']['c_proj']['bias']\n",
    "\n",
    "        # --- Try 3\n",
    "\n",
    "        # # --- Attention QKV ---\n",
    "        qkv_kernel = hf_block['attn']['c_attn']['kernel'].T  # (768, 2304)\n",
    "        qkv_kernel = qkv_kernel.reshape(768, 3, config.num_heads, config.head_dim)  # (768, 3, 12, 64)\n",
    "        qkv_kernel = np.transpose(qkv_kernel, (0, 2, 1, 3))  # (768, 12, 3, 64)\n",
    "        qkv_kernel = qkv_kernel.reshape(768, config.num_heads, 3 * config.head_dim)  # (768, 12, 192)\n",
    "\n",
    "        qkv_bias = hf_block['attn']['c_attn']['bias'].reshape(3, config.num_heads, config.head_dim)  # (3, 12, 64)\n",
    "        qkv_bias = np.transpose(qkv_bias, (1, 0, 2)).reshape(config.num_heads, 3 * config.head_dim)  # (12, 192)\n",
    "\n",
    "        block[attn_key]['DenseGeneral_0']['kernel'] = qkv_kernel\n",
    "        block[attn_key]['DenseGeneral_0']['bias'] = qkv_bias\n",
    "    \n",
    "        # --- Attention output projection ---\n",
    "        block[attn_key]['Dense_0']['kernel'] = hf_block['attn']['c_proj']['kernel'].T\n",
    "        block[attn_key]['Dense_0']['bias'] = hf_block['attn']['c_proj']['bias']\n",
    "\n",
    "        # --- MLP ---\n",
    "        block[mlp_key]['Dense_0']['kernel'] = hf_block['mlp']['c_fc']['kernel'].T\n",
    "        block[mlp_key]['Dense_0']['bias'] = hf_block['mlp']['c_fc']['bias']\n",
    "        block[mlp_key]['Dense_1']['kernel'] = hf_block['mlp']['c_proj']['kernel'].T\n",
    "        block[mlp_key]['Dense_1']['bias'] = hf_block['mlp']['c_proj']['bias']\n",
    "    \n",
    "    # Final layer norm\n",
    "    params['LayerNorm_0']['scale'] = hf_params['transformer']['ln_f']['scale']\n",
    "    params['LayerNorm_0']['bias'] = hf_params['transformer']['ln_f']['bias']\n",
    "    state = state.replace(params=params)\n",
    "\n",
    "    assert np.allclose(\n",
    "        state.params['token_embed']['embedding'],\n",
    "        hf_params['transformer']['wte']['embedding']\n",
    "    )\n",
    "    \n",
    "    del hf_model\n",
    "    del hf_params\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ca04fea-3cd2-4bb9-a280-64798d3ae2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def _compute_metrics(*, state, batch):\n",
    "    preds = state.apply_fn({'params': state.params}, batch[0], train=False)\n",
    "    loss = loss_fn(preds, batch[1]).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(\n",
    "        logits=preds, labels=batch[1], loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a09ed9b5-aa57-4e9c-8718-94529a54ca9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output dim (1, 1023, 50257)\n",
      "num params 124439808\n",
      "Training model 250502-1121_wiki2_1cl_100000_10000_Transformer_L12_H768_Heads12_seed0_sgdFam_1b0.9_2b0.99_3b0.0_lr0.005_warmup2_epochs10_bs1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66855545c4d642459f057411599dc2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 11:23:55.096774: W external/xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 2.46GiB (2647627154 bytes) by rematerialization; only reduced to 3.51GiB (3772495596 bytes), down from 4.38GiB (4707561204 bytes) originally\n",
      "2025-05-02 11:24:07.651657: W external/xla/xla/tsl/framework/bfc_allocator.cc:497] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.42GiB (rounded to 1521532928)requested by op \n",
      "2025-05-02 11:24:07.655340: W external/xla/xla/tsl/framework/bfc_allocator.cc:508] *******************____________***************************************************************x*****\n",
      "E0502 11:24:07.656124 2402372 pjrt_stream_executor_client.cc:3084] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1521532872 bytes.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1521532872 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 83\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_train:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m\n\u001b[1;32m     84\u001b[0m experiment_name, lse \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mfind_latest_exp(experiment_name, n_epochs, save_freq\u001b[38;5;241m=\u001b[39mcb_freq, \n\u001b[1;32m     85\u001b[0m                                            cbs\u001b[38;5;241m=\u001b[39mcb_name_list, unknown_lse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msharp: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meigvals\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_fb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtqdm_over_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtqdm_over_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m         \n\u001b[1;32m     99\u001b[0m all_mh\u001b[38;5;241m.\u001b[39mappend(metrics_history)\n\u001b[1;32m    100\u001b[0m all_exp_names\u001b[38;5;241m.\u001b[39mappend(experiment_name)\n",
      "File \u001b[0;32m/mnt/sdb2/code/jax_eigs/training.py:289\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(state, model, loss_fn, metrics_history, n_epochs, loaders, name, callbacks, option, tqdm_over_epochs, tqdm_over_batch, force_fb, verbose, tqdm_maxinterval, eval_freq, gradient_accumulation)\u001b[0m\n\u001b[1;32m    286\u001b[0m accum_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_bar:\n\u001b[0;32m--> 289\u001b[0m     state, batch_grads \u001b[38;5;241m=\u001b[39m \u001b[43m_get_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# get updated train state (which contains the updated parameters)\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     grads \u001b[38;5;241m=\u001b[39m _tree_add(grads, batch_grads)\n\u001b[1;32m    291\u001b[0m     accum_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/jax_transformer/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1288\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1286\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1288\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1291\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1521532872 bytes."
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "import training\n",
    "importlib.reload(training)\n",
    "\n",
    "load_files = False\n",
    "compute_hessian = False\n",
    "force_train = True\n",
    "\n",
    "all_mh = []\n",
    "all_exp_names = []\n",
    "\n",
    "for hyp in hyp_list:\n",
    "    \n",
    "    metrics_history = {'train_loss': [],\n",
    "                   'train_accuracy': [],\n",
    "                   'train_perplexity': [],\n",
    "                   'test_loss': [],\n",
    "                   'test_accuracy': [],\n",
    "                   'test_perplexity': [],\n",
    "                      }\n",
    "\n",
    "    lr, b1, b2, b3, sam, sam_rho, sync_T = hyp[0]\n",
    "    seed = hyp[1]\n",
    "    option = 'sam' if sam else \"\"\n",
    "    \n",
    "    if datasets is None:\n",
    "        datasets, data_name = __get_datasets__()\n",
    "    \n",
    "    train_loader = lib_data.NumpyLoader(datasets[0], batch_size=bs, shuffle=False)\n",
    "    for sample_batch in train_loader:\n",
    "        break\n",
    "    \n",
    "    test_loader = lib_data.NumpyLoader(datasets[1], batch_size=eval_bs)\n",
    "    dataloaders = [train_loader, test_loader]\n",
    "    \n",
    "    model, model_name = __get_arch__()\n",
    "    model_name += \"_seed\"+str(seed)\n",
    "\n",
    "    optim, optim_name = __get_optim__(warmup_steps, lr, b1, b2, b3, option=option, rho=sam_rho, sync_period=sync_T)\n",
    "    optim_name += f\"_epochs{n_epochs}_bs{bs}\"\n",
    "\n",
    "    init_rng = jax.random.PRNGKey(seed)\n",
    "    state = create_train_state(model, optim, sample_batch[0], init_rng, option=option)\n",
    "    del init_rng  # Must not be used anymore.\n",
    "\n",
    "    state = load_params(state)\n",
    "    \n",
    "    sample_out = state.apply_fn({'params': state.params,}, sample_batch[0], train=False)\n",
    "    print(\"output dim\", sample_out.shape)\n",
    "\n",
    "    # break\n",
    "    num_params = utils.count_params(state.params)\n",
    "    print(\"num params\", num_params)\n",
    "    \n",
    "    # evaluate perplexity\n",
    "\n",
    "    # train_bar = tqdm(train_loader, desc='train', total=len(train_loader))\n",
    "    # test_bar = tqdm(test_loader, desc=\"validation\", total=len(test_loader))\n",
    "\n",
    "    # for batch in train_bar:\n",
    "    #     state = _compute_metrics(state=state, batch=batch)\n",
    "    # for metric, value in state.metrics.compute().items():  # compute metrics\n",
    "    #     metrics_history[f'train_{metric}'].append(value)  # record metrics\n",
    "    # utils.reset_metrics(state)\n",
    "    \n",
    "    # for batch in test_bar:\n",
    "    #     state = _compute_metrics(state=state, batch=batch)\n",
    "    # for metric, value in state.metrics.compute().items():  # compute metrics\n",
    "    #     metrics_history[f'test_{metric}'].append(value)  # record metrics\n",
    "    # utils.reset_metrics(state)\n",
    "    # print(metrics_history)\n",
    "    \n",
    "    # train model\n",
    "    cbs = __get_cbs__(state, compute_hessian=compute_hessian)\n",
    "    cb_name_str = utils.get_callback_name_str(cbs)\n",
    "    cb_name_list = utils.get_callback_name_list(cbs)\n",
    "    # break\n",
    "\n",
    "    experiment_name = utils.get_now() + \"_\" + data_name + \"_\" + model_name + \"_\" + optim_name\n",
    "    \n",
    "    try:\n",
    "        if force_train:\n",
    "            raise FileNotFoundError\n",
    "        experiment_name, lse = utils.find_latest_exp(experiment_name, n_epochs, save_freq=cb_freq, \n",
    "                                                   cbs=cb_name_list, unknown_lse=True, verbose=False)\n",
    "        metrics_history = utils.load_thing(\"traj/\" + experiment_name + \"/metrics.pkl\")\n",
    "        print(f\"tr_acc: {metrics_history['train_accuracy'][-1]:0%}, te_acc: {metrics_history['test_accuracy'][-1]:0%}\")\n",
    "        metrics_history['lse'] = [lse]\n",
    "        if compute_hessian:\n",
    "            eigvals = utils.load_thing(\"traj/\" + experiment_name + \"/eigvals.pkl\")\n",
    "            metrics_history['eigvals'] = eigvals\n",
    "            print(f\"sharp: {metrics_history['eigvals'][-1][0]}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        metrics_history = training.train_model(state, model, loss_fn, metrics_history, n_epochs, dataloaders, \\\n",
    "                                                   experiment_name, cbs, option=option, force_fb=False, tqdm_over_epochs=1, \n",
    "                                              eval_freq=1, gradient_accumulation=32, tqdm_over_batch=False)         \n",
    "        \n",
    "    all_mh.append(metrics_history)\n",
    "    all_exp_names.append(experiment_name)\n",
    "    \n",
    "    print(experiment_name, \"complete\")\n",
    "    print(\"\\n ---------------------------------------------------------------------------------------------------------\\n\")\n",
    "# Training: datasets, hps, arch_func, optim_func, cb_func, -> train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b6f25d-349d-45b0-877b-c5fd43f1b000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee92ee6-f8fa-4ecd-96ac-9db481096fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218f96a-83d8-41ba-913e-b02344bc19fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71eb9178-e47b-45cf-ada0-bbbc84bc7342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output dim (1, 127, 50257)\n",
      "num params 124439808\n",
      "Training model 250501-1848_wiki2_1cl_10_10_Transformer_L12_H768_Heads12_seed0_sgdFam_1b0.9_2b0.99_3b0.0_lr0.005_warmup2_epochs0_bs1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73882d6855a43acab7c2fac39bd0da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete 250501-1848_wiki2_1cl_10_10_Transformer_L12_H768_Heads12_seed0_sgdFam_1b0.9_2b0.99_3b0.0_lr0.005_warmup2_epochs0_bs1\n",
      "250501-1848_wiki2_1cl_10_10_Transformer_L12_H768_Heads12_seed0_sgdFam_1b0.9_2b0.99_3b0.0_lr0.005_warmup2_epochs0_bs1 complete\n",
      "\n",
      " ---------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "import training\n",
    "importlib.reload(training)\n",
    "\n",
    "load_files = False\n",
    "compute_hessian = False\n",
    "force_train = True\n",
    "\n",
    "all_mh = []\n",
    "all_exp_names = []\n",
    "\n",
    "for hyp in hyp_list:\n",
    "    \n",
    "    metrics_history = {'train_loss': [],\n",
    "                   'train_accuracy': [],\n",
    "                   'train_perplexity': [],\n",
    "                   'test_loss': [],\n",
    "                   'test_accuracy': [],\n",
    "                   'test_perplexity': [],\n",
    "                      }\n",
    "\n",
    "    lr, b1, b2, b3, sam, sam_rho, sync_T = hyp[0]\n",
    "    seed = hyp[1]\n",
    "    option = 'sam' if sam else \"\"\n",
    "    \n",
    "    if datasets is None:\n",
    "        datasets, data_name = __get_datasets__()\n",
    "    \n",
    "    train_loader = lib_data.NumpyLoader(datasets[0], batch_size=bs, shuffle=True)\n",
    "    for sample_batch in train_loader:\n",
    "        break\n",
    "    \n",
    "    test_loader = lib_data.NumpyLoader(datasets[1], batch_size=eval_bs)\n",
    "    dataloaders = [train_loader, test_loader]\n",
    "    \n",
    "    model, model_name = __get_arch__()\n",
    "    model_name += \"_seed\"+str(seed)\n",
    "\n",
    "    optim, optim_name = __get_optim__(warmup_steps, lr, b1, b2, b3, option=option, rho=sam_rho, sync_period=sync_T)\n",
    "    optim_name += f\"_epochs{n_epochs}_bs{bs}\"\n",
    "\n",
    "    init_rng = jax.random.PRNGKey(seed)\n",
    "    state = create_train_state(model, optim, sample_batch[0], init_rng, option=option)\n",
    "    del init_rng  # Must not be used anymore.\n",
    "\n",
    "    state = load_params(state)\n",
    "    \n",
    "    sample_out = state.apply_fn({'params': state.params,}, sample_batch[0], train=False)\n",
    "    print(\"output dim\", sample_out.shape)\n",
    "    # break\n",
    "    cbs = __get_cbs__(state, compute_hessian=compute_hessian)\n",
    "    cb_name_str = utils.get_callback_name_str(cbs)\n",
    "    cb_name_list = utils.get_callback_name_list(cbs)\n",
    "    # break\n",
    "    num_params = utils.count_params(state.params)\n",
    "    print(\"num params\", num_params)\n",
    "\n",
    "    experiment_name = utils.get_now() + \"_\" + data_name + \"_\" + model_name + \"_\" + optim_name\n",
    "    \n",
    "    try:\n",
    "        if force_train:\n",
    "            raise FileNotFoundError\n",
    "        experiment_name, lse = utils.find_latest_exp(experiment_name, n_epochs, save_freq=cb_freq, \n",
    "                                                   cbs=cb_name_list, unknown_lse=True, verbose=False)\n",
    "        metrics_history = utils.load_thing(\"traj/\" + experiment_name + \"/metrics.pkl\")\n",
    "        print(f\"tr_acc: {metrics_history['train_accuracy'][-1]:0%}, te_acc: {metrics_history['test_accuracy'][-1]:0%}\")\n",
    "        metrics_history['lse'] = [lse]\n",
    "        if compute_hessian:\n",
    "            eigvals = utils.load_thing(\"traj/\" + experiment_name + \"/eigvals.pkl\")\n",
    "            metrics_history['eigvals'] = eigvals\n",
    "            print(f\"sharp: {metrics_history['eigvals'][-1][0]}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        metrics_history = training.train_model(state, model, loss_fn, metrics_history, n_epochs, dataloaders, \\\n",
    "                                                   experiment_name, cbs, option=option, force_fb=False, tqdm_over_epochs=1, \n",
    "                                              eval_freq=1, gradient_accumulation=1)         \n",
    "        \n",
    "    all_mh.append(metrics_history)\n",
    "    all_exp_names.append(experiment_name)\n",
    "    \n",
    "    print(experiment_name, \"complete\")\n",
    "    print(\"\\n ---------------------------------------------------------------------------------------------------------\\n\")\n",
    "# Training: datasets, hps, arch_func, optim_func, cb_func, -> train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba3000e4-7a4f-46fb-86db-389d1f8e5ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': [Array(4.0963697, dtype=float32)], 'train_accuracy': [Array(0.28031495, dtype=float32)], 'train_perplexity': [Array(60.12163, dtype=float32)], 'test_loss': [Array(4.162267, dtype=float32)], 'test_accuracy': [Array(0.32283464, dtype=float32)], 'test_perplexity': [Array(64.21695, dtype=float32)], 'lse': 0}\n"
     ]
    }
   ],
   "source": [
    "print(all_mh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18481a5c-d4b0-4994-9ff1-6fba0e8354bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " of the is not\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "input_ids = tokenizer(\"The meaning of life is\", return_tensors=\"jax\").input_ids\n",
    "out = state.apply_fn({'params': state.params}, input_ids, train=False)\n",
    "decoded = tokenizer.decode(jnp.argmax(out, axis=-1)[0])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5d41d63-7316-4bb0-9d71-3d2425ec795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 127)\n",
      " game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi\n"
     ]
    }
   ],
   "source": [
    "# print(sample_out.shape)\n",
    "token_ids = sample_batch[0]\n",
    "# token_ids = jnp.argmax(nn.softmax(sample_out), axis=-1)  # shape: [batch_size, seq_len]\n",
    "print(token_ids.shape)\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "decoded = tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n",
    "for text in decoded:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9f335bb-85e6-4c44-ad9f-5e47b2ec6e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game            | ,\n",
      "and             | the\n",
      "follows         | the\n",
      "the             | storyDco\n",
      "\"               | Man\n",
      "Nameless        | series\n",
      "\"               | \"\n",
      ",               | characterized\n",
      "a               | unit\n",
      "penal           | that\n",
      "military        | in\n",
      "unit            | United's\n",
      "serving         | Namia.\n",
      "the             | the\n",
      "nation          | Second\n",
      "of              | Worldvision\n",
      "Gallia          | War.\n",
      "during          | were\n",
      "the             | a\n",
      "Second          | operations\n",
      "Europan         | ops\n",
      "War             | to\n",
      "who             | are\n",
      "perform         | responsible\n",
      "secret          | against\n",
      "black           | each\n",
      "operations      | \"\n",
      "and             | Guard\n",
      "are             | of\n",
      "pitted          | Theamity\n",
      "against         | \"\n",
      "the             | \"\n",
      "Imperial        | \"\n",
      "unit            | (\n",
      "\"               | \"\n",
      "Calamaty        | is\n",
      "Raven           | with\n",
      "\"               | in\n",
      ".               | 2003\n",
      "The             | and\n",
      "game            | and\n",
      "began           | out\n",
      "development     | from\n",
      "in              | year\n",
      "2010            | amount\n",
      ",               | of\n",
      "carrying        | the\n",
      "over            | game\n",
      "a               | from\n",
      "large           | by\n",
      "portion         | thealkyria\n",
      "of              | Chronicles\n",
      "the             | 2\n",
      "work            | .\n",
      "done            | The\n",
      "on              | the\n",
      "Valkyria        | was\n",
      "Chronicles      | the\n",
      "II              | original\n",
      ".               | gameplay\n",
      "While           | of\n",
      "it              | the\n",
      "retained        | original,\n",
      "the             | the\n",
      "standard        | was\n",
      "features        | included\n",
      "of              | some\n",
      "the             | changes\n",
      "series          | to\n",
      ",               | including\n",
      "it              | as\n",
      "also            | the\n",
      "underwent       | the\n",
      "multiple        | game\n",
      "adjustments     | more\n",
      ",               | difficult\n",
      "such            | and\n",
      "as              | the\n",
      "making          | newcomers.\n",
      "the             | development\n",
      "game            | andyoa\n",
      "more            | Kkou\n",
      "forgiving       | also\n",
      "for             | game\n",
      "series          | Toshi\n",
      "newcomers       | Nakamoto\n",
      ".               | also\n",
      "Character       | worked\n",
      "designer        | to\n",
      "Raita           | the\n",
      "Honjou          | projects\n",
      "and             | in\n",
      "composer        | and\n",
      "Hitoshi         | with\n",
      "Sakimoto        | thealkyria\n",
      "both            | Chronicles\n",
      "returned        | II\n",
      "from            | composer\n",
      "previous        | Mashi\n",
      "entries         | K\n",
      ",               | \n",
      "along           | \n",
      "with            | \n",
      "Valkyria        | \n",
      "Chronicles      | \n",
      "II              | \n",
      "director        | \n",
      "Takeshi         | \n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# token_ids = sample_batch[0]\n",
    "token_ids_in = sample_batch[0]\n",
    "token_ids_out = jnp.argmax(nn.softmax(sample_out), axis=-1)  # shape: [batch_size, seq_len]\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "decoded_in = tokenizer.batch_decode(token_ids_in, skip_special_tokens=True)\n",
    "decoded_out = tokenizer.batch_decode(token_ids_out, skip_special_tokens=True)\n",
    "# print(decoded_in)\n",
    "# print(decoded_out)\n",
    "\n",
    "from itertools import zip_longest\n",
    "\n",
    "for in_str, out_str in zip(decoded_in, decoded_out):\n",
    "    in_words = in_str.split()\n",
    "    out_words = out_str.split()\n",
    "    for in_word, out_word in zip_longest(in_words, out_words, fillvalue=\"\"):\n",
    "        print(f\"{in_word:<15} | {out_word}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daae6c2-a912-4e1f-9da3-d5fa09645c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f874861-93ee-4838-9e62-97d5270889e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f582c1f4-f817-45c0-9327-56b93b213020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 39\n"
     ]
    }
   ],
   "source": [
    "print(len(optim_hp_list), len(all_mh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08373c1c-b5ad-4dbd-ac78-5024bf6e9bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD, train_accuracy:0.9998698234558105, test_accuracy:0.5381667017936707, lse:60.333333333333336\n",
      "SGD-SAM, train_accuracy:0.9996744990348816, test_accuracy:0.5221666693687439, lse:62.0\n",
      "ADAM, train_accuracy:0.9994141459465027, test_accuracy:0.5755000114440918, lse:1326.3333333333333\n",
      "ADAM-SAM-R0, train_accuracy:0.9992188215255737, test_accuracy:0.5898333787918091, lse:1417.3333333333333\n",
      "ADAM-SAM, train_accuracy:0.9994140863418579, test_accuracy:0.5566667318344116, lse:1307.0\n",
      "ADAM-UB-1e0, train_accuracy:0.9991536140441895, test_accuracy:0.5726667642593384, lse:1045.6666666666667\n",
      "ADAM-UB-1e0-SAM, train_accuracy:0.9995443224906921, test_accuracy:0.5693333745002747, lse:1139.0\n",
      "ADAM-UB-5e-1, train_accuracy:0.9994140863418579, test_accuracy:0.5721666812896729, lse:1001.0\n",
      "ADAM-UB-1e-1, train_accuracy:0.9996744990348816, test_accuracy:0.5730000734329224, lse:1139.6666666666667\n",
      "ADAM-UB-5e-2, train_accuracy:0.9992188215255737, test_accuracy:0.561333417892456, lse:889.0\n",
      "ADAM-UB-1e-2, train_accuracy:0.9994141459465027, test_accuracy:0.5073333978652954, lse:624.6666666666666\n",
      "ADAM-UB-5e-3, train_accuracy:0.7018880248069763, test_accuracy:0.3358333706855774, lse:357.6666666666667\n",
      "ADAM-UB-1e-3, train_accuracy:0.1070963591337204, test_accuracy:0.10200001299381256, lse:100.0\n"
     ]
    }
   ],
   "source": [
    "optim_names = ['SGD', 'SGD-SAM', 'ADAM', 'ADAM-SAM-R0', 'ADAM-SAM', 'ADAM-UB-1e0', 'ADAM-UB-1e0-SAM', 'ADAM-UB-5e-1','ADAM-UB-1e-1', 'ADAM-UB-5e-2', 'ADAM-UB-1e-2', 'ADAM-UB-5e-3','ADAM-UB-1e-3']\n",
    "stat_names = ['train_accuracy', 'test_accuracy', 'lse']\n",
    "for i in range(len(optim_hp_list)):\n",
    "    stats = [ 0 for j in range(len(stat_names))]\n",
    "    for j in range(len(stats)):\n",
    "        for k in range(3*i, 3*i+3):\n",
    "            stats[j] += all_mh[k][stat_names[j]][-1]\n",
    "    out = f\"{optim_names[i]}\"\n",
    "    for j in range(len(stats)):\n",
    "        out += f\", {stat_names[j]}:{stats[j]/3}\"\n",
    "    print(out)\n",
    "    # print(optim_hp_list[i], np.mean(all_mh[3*i:3*i+3]['train_accuracy'][-1]), np.mean(all_mh[3*i:3*i+3]['test_accuracy'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d0fcc8-c67d-4817-baa5-f39b980fa980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(all_mh)):\n",
    "    print(hyp_list[i][0], all_mh[i]['train_accuracy'][-1], all_mh[i]['test_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb7171db-c859-483f-b386-fbdd7084cb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1, 0.0, 0.0, 0.0, False, 0.0, 1) 0.99921876 0.508\n",
      "(0.1, 0.0, 0.0, 0.0, True, 0.1, 1) 0.99902344 0.5245\n",
      "(0.005, 0.9, 0.99, 0.0, False, 0.0, 1) 0.99921876 0.5705\n",
      "(0.005, 0.9, 0.99, 0.0, True, 0.0, 1) 0.99902344 0.583\n",
      "(0.005, 0.9, 0.99, 0.0, True, 0.001, 1) 0.99902344 0.57750005\n",
      "(0.005, 0.9, 0.99, -1.0, False, 0.0, 1) 0.9996094 0.586\n",
      "(0.005, 0.9, 0.99, -1.0, True, 0.001, 1) 0.9996094 0.56450003\n",
      "(0.005, 0.9, 0.99, -0.5, False, 0.0, 1) 0.9996094 0.5755\n",
      "(0.005, 0.9, 0.99, -0.1, False, 0.0, 1) 0.9998047 0.573\n",
      "(0.005, 0.9, 0.99, -0.05, False, 0.0, 1) 0.9994141 0.58100003\n",
      "(0.005, 0.9, 0.99, -0.01, False, 0.0, 1) 0.99902344 0.54700005\n",
      "(0.005, 0.9, 0.99, -0.005, False, 0.0, 1) 0.9142578 0.3535\n",
      "(0.005, 0.9, 0.99, -0.001, False, 0.0, 1) 0.09980469 0.108500004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(all_mh)):\n",
    "    print(hyp_list[i][0], all_mh[i]['train_accuracy'][-1], all_mh[i]['test_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b06a76-3cca-4ade-8291-16fcf5250747",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
